<!DOCTYPE html>
<html>
  <head>
    <title>Open, Reliable & Repeatable :: CDT 2/04/2025</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="../assets/asciinema-player/asciinema-player.css" />
    <link rel="stylesheet" type="text/css" href="../css/slides.css" />
  </head>
  <body>
<script src="../assets/asciinema-player/asciinema-player.min.js"></script>
<script src="https://kit.fontawesome.com/015d4b01de.js" crossorigin="anonymous"></script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>


<textarea id="source">
name: title
layout: true
class: center
---
layout: false
count: false


.center[
<a href="https://oesteban.github.io/talks/NEMES-GU-2025/">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 20%"></object>
  <br />
  https://oesteban.github.io/talks/NEMES-GU-2025/
</a>

<br />
<br />

## Open, Reliable, and Repeatable

### Foundations for Breakthroughs in Connectivity Analyses
<br />

Oscar Esteban &lt;<code><a href="mailto:phd@oscaresteban.es">phd@oscaresteban.es</a></code>>

#### NEMES Göteborg, Sep 25<sup>th</sup>, 2025
]

???


---
name: newsection
layout: true

.perma-sidebar[
<p class="rotate">
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0; height: 20px; padding-top: 6px;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
  <span style="padding-left: 10px; font-weight: 600;">Open, Reliable & Repeatable :: CDT 2/04/2025</span>
</p>
]

---

# About me

.large[
**Computational neuroscientist** & **open science** advocate.
]

.right-column3.center[
<a href="https://oesteban.github.io/talks/NEMES-GU-2025/">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 80%"></object>
  <br />
  Link to slides
</a>
]

.pad-top.left-column3[
.people-table.larger[

| | |
|---:|---|
| ![oscar-esteban](https://www.axonlab.org/images/teampic/OscarEsteban-300x300.jpg) | **Oscar Esteban** <br /> Research & Teaching FNS Fellow <br /> Head of [AxonLab](https://www.axonlab.org) <br /> Dept. of Radiology, CHUV |
]


.larger[
* PD (2020) @ Stanford University
* Ph.D. (2015) @ Universidad Politécnica de Madrid <br />ESKAS (2012) @ EPFL
]

]

---

<br />

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right"></i> .larger[**Context**]
  <br /> .indent[.gray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right"></i> .larger[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.gray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right"></i> .larger[**Quality assurance & control (QA/QC)**]
  <br /> .indent[.gray-text[Opening windows into your data]]

* <i class="fa-solid fa-circle-right"></i> .larger[**Standardization**]
  <br /> .indent[.gray-text[Registered reports, SOPs & NiPreps]]
]

???

---
count:false

<br />

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right"></i> .larger[**Context**]
  <br /> .indent[.gray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Quality assurance & control (QA/QC)**]
  <br /> .indent[.dimgray-text[Opening windows into your data]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Standardization**]
  <br /> .indent[.dimgray-text[Registered reports, SOPs & NiPreps]]
]

???

---

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 100%; padding-top: 20pt;"></object>

.center[
[Esteban et al., (2020)](http://doi.org/10.1038/s41596-020-0327-3);
[Niso et al., (2022)](https://doi.org/10.1016/j.neuroimage.2022.119623)
]
]

???

In broad strokes, this is the MRI-based connectivity analysis pipeline.

Interestingly, it looks like any machine learning pipeline: we acquire data, perform quality control, preprocess it, define features, and then apply statistical modeling to extract insights.

For a deeper dive into the details of the neuroimaging worflow, I've participated in several efforts such as the two references given below.

---
count:false

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-1.svg" style="width: 100%; padding-top: 20pt;"></object>

.center[
[Esteban et al., (2017)](https://doi.org/10.1371/journal.pone.0184661);
[Provins et al., (2023)](https://doi.org/10.3389/fnimg.2022.1073734); <br />
[Provins et al., (2024, *Stage 2 RR under review*)](https://doi.org/10.1101/2024.10.11.617777)
[Hagen et al., (2024, *under review*)](https://doi.org/10.1101/2024.10.21.619532)
]
<br />

.no-bullet[
* .larger[<i class="fa-solid fa-clipboard-list"></i> Run the **MRI experiment** following Standard Operating Procedures (SOPs)]

* <i class="fa-solid fa-folder-tree"></i> .larger[**Standardized data structure** (BIDS—Brain Imaging Data Structure)]

* .larger[<i class="fa-solid fa-square-check"></i> **QA/QC** (Quality assurance / control)]

]

]

???

The pipeline starts with data acquisition, management and quality assurance and control (QA/QC).

My early work on QA/QC applied machine learning to automatically assess MRI images of the brain, and raised awareness over issues like “site effects”—where data from different scanners or labs can systematically vary, much like “batch effects” in genomics.

One key principle in AI is that models are only as good as their data and neuroimaging is no exception.

This is why my group and I have been deeply involved in BIDS, produced version-controlled, machine-readable standard operating procedures, developed comprehensive QA/QC protocols and created ready-to-use applications to detect suboptimal data early.

---
count:false

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-2.svg" style="width: 100%; padding-top: 20pt;"></object>

.center[
[Esteban et al., (2019)](https://doi.org/10.1038/s41592-018-0235-4);
[Ciric et al., (2022)](https://doi.org/10.1038/s41592-022-01681-2);
[Adebimpe et al., (2022)](https://doi.org/10.1038/s41592-022-01458-7)
]
<br />

.no-bullet[
* .larger[<i class="fa-solid fa-smog"></i> Detection of **nuisance sources**]

* .larger[<i class="fa-solid fa-magnifying-glass-location"></i> Spatiotemporal **location** of signals]

* .larger[<i class="fa-solid fa-location-crosshairs"></i> Definition of **brain units** of analysis (regions)]

]
]

???

Next in the pipeline is preprocessing, which transforms raw data into something models can reliably interpret.

For example, it identifies signals of no interest for their cleaning or accounting within modeling.

In neuroimaging, preprocessing also deals with accurately locating signals and objects in space and time, and often the definition of relevant brain regions that focus the analysis.

A decade ago, we might have called it “feature engineering.”

Regardless of the term, it’s crucial: “preprocessing” is now a keyword in job posts at major players like Anthropic, DeepMind, and OpenAI.

---

# fMRIPrep: despite minimal, a complex workflow

.boxed-content.center[
<img src="https://github.com/oesteban/fmriprep/raw/f4c7a9804be26c912b24ef4dccba54bdd72fa1fd/docs/_static/fmriprep-21.0.0.svg" style="width: 100%; padding-top: 20pt" />
]

???

Here I'm showing a very much simplified overview of fMRIPrep's design.

Despite it being just "minimal" preprocessing and just one step of the pipeline, its complexity is evident.

Rather than delving into each of these steps, I'll just say that there's a current need to update and improve computer vision algorithms and approaches (segmentation, image registration, dimensionality reduction, etc.) with AI/ML all across the dataflow.

---

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-3.svg" style="width: 100%; padding-top: 20pt;"></object>

.center[
[Thompson et al., (2021, *under review*)](https://doi.org/10.1101/2021.01.16.426941);
[Rodrigue et al., (2021)](https://doi.org/10.1016/j.bpsc.2020.12.002);
[Li et al., (2024)](https://doi.org/10.1038/s41562-024-01942-4)
]
]

.pull-right[
<img src="../assets/matrix-fc.png" alt="matrix-fc" style="width: 20%" />
<br />
**Functional Connectivity (FC)**
<br />
.gray-text[Synchronized BOLD co-variation between brain regions]
]

.pull-left.align-right[
<img src="../assets/matrix-sc.png" alt="matrix-fc" style="width: 20%" />
<br />
**Structural Connectivity (SC)**
<br />
.gray-text[Tracked water diffusion pathways between brain regions]

]

???

Once data are ready for analysis, we formalize connectivity as undirected acyclic graphs, represented with symmetric matrices.

In the case of structural connectivity, we use diffusion MRI to track pathways of white-matter bundles connecting brain regions.

For functional connectivity we measure synchronized functional MRI signal fluctuations sensitive to the dynamic oxygen content in blood.

I’ve been involved in several works applying structural and functional connectivity.

For instance, in Rodrigue 2021, we explored ML approaches to identify clinical biomarkers.

Unfortunately, we had to conclude that, despite promising data quality and reliability remain insufficient for robust biomarker discovery.

---

# Despite active research, scarce clinical application

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 100%; padding-bottom: 10pt; padding-top: 20pt;"></object>
]

.no-bullet[
* .large[<i class="fa-solid fa-triangle-exclamation"></i> Alarming **analytical variability**:]
  * .larger[Diffusion MRI [[Maier-Hein et al., (2015)](https://doi.org/10.1038/s41467-017-01285-x)]]
  * .larger[Functional MRI [[Botvinik-Nezer et al., (2020)](https://doi.org/10.1038/s41586-020-2314-9)]]

<br />
* .large[<i class="fa-solid fa-dna"></i> **Like genomics** between HGP (2003) and NGS (2015)?]
]
]

???

Therefore, although simple on the surface, the workflow is highly-complex and each step introduces potential variability.

Complicating matters further, every lab around the world might use a slightly different pipeline.

Different tools, different parameters—lead to inconsistent results.

This fragmentation hinders progress and precludes between-study comparisons.

The consequence is an acute scarcity of real-world applications of connectivity analyses.

Let's compare to genomics.

Neuroimaging is today in an early stage of standardization akin to genomics at the conclusion of the Human Genome Project in 2003.

One key aspect for the remarkable advance of genomics is precisely the comprehensive standardization process of the pipeline that culminated some 10 years ago.

For that reason, I advocate for replicating this success experience in neuroscience.

---

# Analytical variability in dMRI

<figure style="width: 70%">
![:img ismrm-challenge, 100%](images/tractography-challenge.png)
<figcaption>From <a href="https://doi.org/10.1038/s41467-017-01285-x">Maier-Hein et al. 2017</a>;
doi:![:doi](10.1038/s41467-017-01285-x)</figcaption>
</figure>

---
count:false

# Analytical variability in dMRI

.vertical-center[
.center.white-bg[
.large[
***Tractography-based connectomes are dominated by false-positive connections***
]

Draft title (2015) - doi:[10.1101/084137](https://doi.org/10.1101/084137)

]
]
<div style="background-image: url(images/tractography-challenge.png); background-size: cover; height: 80%; width: 80%; margin-left: 180px; opacity: 0.1">
</div>

---

# Analytical variability in fMRI

<figure style="width: 65%">
![:img narps, 100%](images/narps-paper.png)
<figcaption>From <a href="https://doi.org/10.1038/s41586-020-2314-9">Botvinik-Netzer et al. 2020</a>;
doi:![:doi](10.1038/s41586-020-2314-9)</figcaption>
</figure>

---
count: false

# Despite active research, no real-world application

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 100%; padding-bottom: 10pt; padding-top: 20pt;"></object>
]

.no-bullet[
* .large[<i class="fa-solid fa-triangle-exclamation"></i> Alarming **analytical variability**:]
  * .larger[Diffusion MRI [[Maier-Hein et al., (2015)](https://doi.org/10.1038/s41467-017-01285-x)]]
  * .larger[Functional MRI [[Botvinik-Nezer et al., (2020)](https://doi.org/10.1038/s41586-020-2314-9)]]

<br />
* .large[<i class="fa-solid fa-magnifying-glass-plus"></i> Insufficient **test-retest reliability** at the finer scales of brain units]
  * .larger[[Nakuci et al., (2023)](https://doi.org/10.1038/s41598-023-33441-3)]]
]

???

On top of all that methodological variability, there’s also inherent unreliability in measuring connectivity from session to session.

If you scan the same participant twice, back-to-back, you can still get different connectivity measurements, indicating limited test-retest reliability.

Despite all these concerns, more than 20 thousand papers were published focusing on functional connectivity and 10 thousand on structural connectivity in 2024, as per Pubmed.

---

.center[
<object type="image/svg+xml" data="../assets/reproducible-definition-grid.svg" style="width: 68%"></object>
.small[
*The Turing Way project* illustration by Scriberia. Used under a CC-BY 4.0 licence. doi:<a href="https://doi.org/10.5281/zenodo.3332807">10.5281/zenodo.3332807</a>.]
]

---

# More reliable measurements

.center[
.boxed-content[
<object type="image/svg+xml" data="../assets/reproducible-definition-grid-axes-0.svg" style="width: 65%;"></object>

[Esteban, (2025)](https://doi.org/10.1007/978-1-0716-4260-3_8) [[Methods for analyzing large neuroimaging datasets](https://doi.org/10.1007/978-1-0716-4260-3)]
]
]

---
count: false

# More reliable measurements

.center[
.boxed-content[
<object type="image/svg+xml" data="../assets/reproducible-definition-grid-axes-1.svg" style="width: 65%;"></object>

[Esteban, (2025)](https://doi.org/10.1007/978-1-0716-4260-3_8) [[Methods for analyzing large neuroimaging datasets](https://doi.org/10.1007/978-1-0716-4260-3)]
]
]

---
count: false

# More reliable measurements

.center[
.boxed-content[
<object type="image/svg+xml" data="../assets/reproducible-definition-grid-axes-3.svg" style="width: 65%;"></object>

[Esteban, (2025)](https://doi.org/10.1007/978-1-0716-4260-3_8) [[Methods for analyzing large neuroimaging datasets](https://doi.org/10.1007/978-1-0716-4260-3)]
]
]

---
count: false

# More reliable measurements

.center[
.boxed-content[
<object type="image/svg+xml" data="../assets/reproducible-definition-grid-axes-2.svg" style="width: 65%;"></object>

[Esteban, (2025)](https://doi.org/10.1007/978-1-0716-4260-3_8) [[Methods for analyzing large neuroimaging datasets](https://doi.org/10.1007/978-1-0716-4260-3)]
]
]

---
count:false

<br />

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  <br /> .indent[.dimgray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right"></i> .larger[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.gray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Quality assurance & control (QA/QC)**]
  <br /> .indent[.dimgray-text[Opening windows into your data]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Standardization**]
  <br /> .indent[.dimgray-text[Registered reports, SOPs & NiPreps]]
]

???

Having deeply focused on the standardization to address analytical variability, in 2020 I started to work on the reliability problem of connectivity analysis.

Tat that moment, I identified that the neuroimaging community was lacking an ultra-dense, calibration-grade dataset to address the problem.

With the support of an Ambizione project, I launched the Human Connectome Phantom project.

The idea is to have a single subject—myself—scanned intensively within a very short time window to minimize brain changes (and hence, serve as a "phantom" reference).

The dataset needed to combine sophisticated functional and structural MR acquisitions, and a wealth of physiological tracking.

---

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Avoid **between-subject** variability.

Short span of time to minimize brain changes (~8 weeks)

* Wave 1: **within-scanner variability**
* Wave 2: **between-scanner variability**

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-00.svg" style="width: 90%;"></object>
]

???

As mentioned, I lent myself as the phantom given the hefty commitment the project involved and the total absence of antecedents of a human being scanned so frequently.


---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
**Anatomy**:
  * T<sub>1</sub>-weighted & T<sub>2</sub>-weighted

**Diffusion MRI** (HARDI):

  * 1.6 × 1.6 × 1.6 [mm<sup>3</sup>]
  * 6 × b = 0 s/mm<sup>2</sup>
  * 274 DWIs <br />137 DWIs non-collinear
    b={700,1000,2000,3000} s/mm<sup>2</sup>

**Functional MRI** (BOLD): <br />(next slide)

***B<sub>0</sub>* fieldmaps**:

  * GRE (2min 38s)
  * 4 × EPI/dMRI (54s)
  * 4 × EPI/fMRI (27s)
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-01.svg" style="width: 90%;"></object>
]

???

First, our study focused on a single scanner at CHUV.

We collected a very comprehensive MRI protocol with technical sophistication to provide a solid foundation to secondary methodological developments.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
**Functional MRI**:

  * 2.2 × 2.2 × 2.2 [mm<sup>3</sup>]
  * TR = 1.6s
  * multi-echo (4 echos)
  * three tasks:
    * QC task (3min 7s)
    * naturalistic movie resting (20min 9s)
    * breath-holding task (6min 0s)

<video width="100%" autoplay loop muted>
  <source src="../assets/mundaka-clip.webm" type="video/webm">
  Your browser does not support the video tag.
</video>

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-03.svg" style="width: 90%;"></object>
]

???
We collected eye-tracking to assess arousal and analyze correlation with other facets such as head motion.
We also obtained a wealth of signals to investigate physiological variability, including expired oxygen and CO2 (which is far from standard practice but needed).

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Collected within **4 weeks**

Following stringent **Standard Operating Procedures (SOPs)**

.center[
<a href="https://www.axonlab.org/hcph-sops">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 80%"></object>
  <br />
  Link to **public** SOPs
</a>
]

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-04.svg" style="width: 90%;"></object>
]

???

This first wave of data was collected in about 3 weeks to ensure minimal changes on the human brain phantom.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
<a href="https://www.axonlab.org/hcph-sops/data-management/intro/">Data Management (SOPs)</a>

* BIDS Conversion & Validation

* QA/QC with *MRIQC*

* DataLad: version control

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-05.svg" style="width: 90%;"></object>
]

???

Data were then standardized, properly stored under version control, and quality-controlled.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
<a href="https://www.axonlab.org/hcph-sops/processing/preprocessing/">Preprocessing (SOPs)</a> with *fMRIPrep* and *dMRIPrep*

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-06.svg" style="width: 90%;"></object>
]

???

We preprocessed the data with NiPreps

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
<a href="https://www.axonlab.org/hcph-sops/processing/functional-connectivity/">Connectivity extraction (SOPs)</a> (mostly *NiLearn*)
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-07.svg" style="width: 90%;"></object>
]

???

Next comes connectivity extraction.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
We collected 36 sessions
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-08.svg" style="width: 90%;"></object>
]

???

And this process yielded at least 36 pairs of structural and functional matrices.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Objective: **testing generalization**

* Clinical-use scanners

* Homogenized protocol

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-09.svg" style="width: 90%;"></object>
]

???

But we also wanted to test agreement between scanners.

To do so, we modified the protocol so that it would be suitable for the two clinical scanners with more limited performance.

We collected 12 more sessions on the research scanner with the homogenized protocol.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
We collected 36 sessions: 12 sessions each scanner (x3)
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-10.svg" style="width: 90%;"></object>
]

???

And 12 more sessions per scanner on the two clinical scanners.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Data will be released fully open (CC-BY), including derivatives.

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-full.svg" style="width: 90%;"></object>
]

???

Finally, we committed to release all materials (data, code, operating procedures, etc.) open.

---

## HCPh: Registered-report driven

.right-column3[
.larger.center[
.large[**Stage 1 RR**] <br />
Nature Methods

<a href="https://doi.org/10.17605/OSF.IO/VAMQ6"><object type="image/svg+xml" data="../ISMRM2024/images/qr-RR.svg" style="width: 85%"></object>
Provins et al., (2023)</a>
]
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-full.svg" style="width: 90%;"></object>
]

???

The study was accepted in October 2023 as a Stage 1 Registered Report at Nature Methods.

The registered report, data collection, and the research was carried out by a large team, with the outstanding role of the Ph.D. student funded by the Ambizione (Celine Provins), and is accessible with the QR code.

By pre-registering, we locked in our hypotheses and methods up front and addressed design issues raised by three reviewers before data collection started.

---

# HCPh: Study Outcomes


.boxed-content.no-bullet.pad-top[

* .large.pad-top[<i class="fa-solid fa-database"></i> One-of-a-kind, open **dataset**]
  .indent[
  .gray-text[Thoroughly documented, with open [Standard Operating Procedures (SOPs)](https://www.axonlab.org/hcph-sops)]
  ]

* .large[<i class="fa-solid fa-chart-bar"></i> Within-scanner **reliability characterization**]
  .indent[
  .gray-text[PCA-based modeling of FC residuals]

  .gray-text[Bayesian modeling of SC]
  ]

* .large[<i class="fa-solid fa-scale-balanced"></i> Between-scanner **agreement**]
  .indent[
  .gray-text[One research scanner vs. two clinical scanners]
  ]

]

???

We are currently concluding the final Stage 2 reporting the results, to be submitted to the journal within the next 3-4 months.

We have collected and preprocessed a unique dataset and publicly released its comprehensive standard operating procedures.

Following our pre-registered commitments, we are currently executing our promised analysis involving principal components analyses of functional connectivity and bayesian modelling of structural connectivity.

Finally, we will investigate how structural and functional connectivity measurements agree if obtained with different devices.

This will yield a baseline measure of how stable connectivity metrics can be if the protocol is maximally controlled.

---

# HCPh: Significance
<br />

.boxed-content.no-bullet[

* .large.pad-top[<i class="fa-solid fa-ruler"></i> Complement standardization with **calibration**]
  .indent[
  .gray-text[Provide normative distributions of SC and FC (prior knowledge)]

  .gray-text[An approach to design *calibrated* studies, potentially with the same or new **human phantoms**]
  ]

* .large[<i class="fa-solid fa-route"></i> Methodological **advance**]
  .indent[
  .gray-text[Removal of physiological variability, artifacts, etc.]

  .gray-text[Between-scanner agreement of measurements (other than connectivity)]
  ]

* .large[<i class="fa-solid fa-building-columns"></i> A **foundational dataset**]
  .indent[
  .gray-text[SC & FC training dataset]

  .gray-text[Fully-open, ideal for courses and training]
  ]

]

???

But the impact of HCPh goes far beyond the analyses committed within the Stage 1 report.

We hope to see new “human phantom” datasets at multiple sites, enabling direct cross-site calibration.

HCPh can also serve as ground truth data for AI-driven motion-correction algorithms, generative modeling, or even real-time scanning protocols.

Essentially, we’re complementing standardization efforts (like NiPreps) with a calibration resource that addresses test-retest and multi-site reproducibility.

---
count:false

<br />

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  <br /> .indent[.dimgray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right"></i> .larger[**Quality assurance & control (QA/QC)**]
  <br /> .indent[.gray-text[Opening windows into your data]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Standardization**]
  <br /> .indent[.dimgray-text[Registered reports, SOPs & NiPreps]]
]

???

---

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 90%; padding-bottom: 55.5pt; padding-top: 120pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-00.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-01.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-02.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-03.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-04.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-05.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---

# QA/QC protocols: 'Swiss-cheese security model'

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol.svg" style="width: 60%; padding-bottom: 10pt; padding-top: 10pt;"></object>

<object type="image/svg+xml" data="../ISMRM2024/images/swiss-cheese.svg" style="width:60%"></object>

.small[
[BenAveling @ wikipedia](https://en.wikipedia.org/wiki/Swiss_cheese_model#/media/File:Swiss_cheese_model_textless.svg)
]
]

]

???

For those with knowledge about security protocols, this approach will surely evoke the Swiss cheese model.

The model assumes that all QC checkpoints will have holes through which data progresses toward analysis.

By layering several QC checkpoints looking at the data in different ways, we make sure that images with potential to bias the results do not make all the way through the workflow.


---

# Does defacing influence QA/QC decisions?

.right-column3[
.larger.center[
.large[**Stage 2 RR**] <br />
(nearly accepted)

<a href="https://doi.org/10.1101/2024.10.11.617777"><object type="image/svg+xml" data="../assets/qr-defacing-stage2-preprint.svg" style="width: 85%;"></object>

Provins et al., (2025)</a>
]
]

.left-column3[
<img src="../assets/defacing-RR-figure1.png" style="width: 90%; padding-top: 25pt" /><br />
]

???

---

# Does defacing influence QA/QC decisions?

.boxed-content[

.pull-right.center[
]

.pull-left[
<br />

.large[**IXI dataset** (N = 581; 3 sites)]
* **Manual QA/QC**: <br style="padding-top: 0" /> N = 185 (Hammersmith Hospital, 3T) <br />4 raters
* **Automated QA/QC**: <br style="padding-top: 0" /> Full dataset <br /> IQMs (image quality metrics)

.large[**Processing**:] <br />
MRIQC reports (**manual**) and IQMs (**automated**)
* before defacing (**nondefaced**)
* after defacing (**defaced**)

([Provins et al., 2025](https://doi.org/10.1101/2024.10.11.617777))
]

]

---

# Does defacing influence **manual** QA/QC decisions?

.boxed-content[

.pull-right.center[
<object type="image/svg+xml" data="../assets/defacing-RR-ba-traditional-minimal.svg" style="width: 100%; padding-top: 17pt"></object>
]

.pull-left[
<br />

.large[**4 raters** assessed 185 images x2]

.large[**Bland-Altman** analysis]

([Provins et al., 2025](https://doi.org/10.1101/2024.10.11.617777))
]

]

---
count: false

# Does defacing influence **manual** QA/QC decisions?

.boxed-content[

.pull-right.center[
<object type="image/svg+xml" data="../assets/defacing-RR-ba-traditional.svg" style="width: 100%; padding-top: 17pt"></object>
]

.pull-left[
<br />

.large[**4 raters** assessed 185 images x2]

.large[**Bland-Altman** analysis]

.larger[
* Large, asymmetric **variability** <br />LoA = [-0.91, 0.81]

* **Negative**—albeit small—bias<br />
  𝔼[Δ<sub>ndef-def</sub>] = -0.05
]


([Provins et al., 2025](https://doi.org/10.1101/2024.10.11.617777))
]

]

---
count: false

# Does defacing influence **manual** QA/QC decisions?

.boxed-content[

.pull-right.center[
<img src="../assets/defacing-RR-ba-animated.gif" alt="defacing-RR" style="width: 100%;" />
]

.pull-left[
<br />

.large[**4 raters** assessed 185 images x2]

.large[**Bland-Altman** analysis]

.larger[
* Large, asymmetric **variability** <br />LoA = [-0.91, 0.81]

* **Negative**—albeit small—bias<br />
  𝔼[Δ<sub>ndef-def</sub>] = -0.05
]

([Provins et al., 2025](https://doi.org/10.1101/2024.10.11.617777))
]

]

---

# Does defacing influence **manual** QA/QC decisions?

.boxed-content[

.pull-right.center[

<object type="image/svg+xml" data="../assets/defacing-RR-ba-optimized.svg" style="width: 100%; padding-top: 17pt"></object>


]

.pull-left[
<br />

.large[
* **Overoptimistic** defaced 

* concentrated at **lowest nondefaced**,

* **more extreme** &

* **more numerous**
]
]

]

---

# Does defacing influence QA/QC decisions?

.boxed-content.center[
<img src="../assets/defacing-RR-figure2.png" style="width: 90%; padding-top: 15pt;" />
]

???

---
count:false

<br />
.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  <br /> .indent[.dimgray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Quality assurance & control (QA/QC)**]
  <br /> .indent[.dimgray-text[Opening windows into your data]]

* <i class="fa-solid fa-circle-right fa-solid"></i> .larger[**Standardization**]
  <br /> .indent[.gray-text[Registered reports, SOPs & NiPreps]]
]

???

---

# **The paradox** (Prof. C. Chambers)

--

<br />
<br />
<br />

## Results determine careers

<br />

--

and

<br />

## Resarchers SHOULD NOT determine results.

---

# What are Registered Reports?

<p align="center">
<img src="../BHD2023/images/RRs-traditional-before-review.png" width="90%" /><br /><br />
</p>

---

# What are Registered Reports?

<p align="center">
<img src="../BHD2023/images/RRs-traditional-after-review.png" width="90%" /><br /><br />
</p>

---

# What are Registered Reports?

<p align="center">
<img src="../BHD2023/images/RRs-workflow.png" width="90%" /><br /><br />
</p>

---

.vertical-center[
.center.white-bg[
.large[
**Results do not matter**
]
]
]

---
count: false

.vertical-center[
.center.white-bg[
.large[
**Results** [*whether positive or negative*] **do not matter**
]
]
]

---

# Why the Registered Report Approach

<br />

.boxed-content.large.no-bullet[

* <i class="fa-solid fa-circle-right"></i> .larger[**Pre-Approved Study Design**]
  <br /> .indent[.gray-text[Hypotheses, protocols, and analyses peer-reviewed before data collection]]

* <i class="fa-solid fa-circle-right"></i> .larger[**Transparency & Rigor**]
  <br /> .indent[.gray-text[Minimizes biases, “p-hacking,” and the garden of forking paths]]

* <i class="fa-solid fa-circle-right"></i> .larger[**Challenges**]
  <br /> .indent[.gray-text[Pre-registration report preparation & slow review process]]
  .indent[.gray-text[Deviations from the canonical confirmatory analysis]]

]

???

Registered Reports reduce biases and ensure transparency as it removes incentives to drive the results and interpretation.

Why? Because with the acceptance of the Stage 1, journals commit to publish the final work (or Stage 2) regardless of the final outcomes.

Given that the HCPh is intended as a foundational reference dataset, it was crucial that the design itself was peer-reviewed.

Registered reports are not free of caveats.

With the communal improvement of scientific endeavors, we will be able to shorten the preparation time of pre-registered analysis and ensure that peer-review does not excessively delays the onset of data collection.

---

# HCPh's SOPs

.boxed-content[
<iframe src="https://www.axonlab.org/hcph-sops/" width="100%" height="510px" style="border: 0; margin-top: 15px"></iframe>
.center.small[https://www.axonlab.org/hcph-sops/]
]

---

# Standardization in neuroimaging—NiPreps

.boxed-content[
<br />
.larger.center[
"*analysis-grade*" data <i class="fa-solid fa-circle-right"></i> data **directly consumable by analyses**
]

.pull-left[
<br />
<br />
*Analysis-grade* data is an analogy to the concept of "*sushi-grade (or [sashimi-grade](https://en.wikipedia.org/wiki/Sashimi)) fish*" in that both are:

.large[<i class="fa-solid fa-circle-right"></i> **minimally preprocessed**,]

and

.large[<i class="fa-solid fa-circle-right"></i> **safe to consume** directly.]
]

.pull-right.center[
<img src="../assets/nipreps-transparent.png" width="100%" />
<a href="https://www.nipreps.org"><object type="text/xml+svg" data="../assets/qr-nipreps-url.svg" style="width: 40%"></object><br />
www.nipreps.org</a>
]
]

???

In computational neuroscience I'm recognized by my standardization efforts.

NiPreps is a framework for standardized preprocessing pipelines in neuroimaging.

I often use the “sushi-grade” analogy

just like sushi-grade fish is minimally processed but safe to consume,

NiPreps yields “analysis-grade” data, with minimal processing interventions but ready for direct machine learning ingestion.

---

# Tracking *fMRIPrep*'s adoption

.center[
<img src="https://www.nipreps.org/assets/20250922_weekly.png" style="width: 90%; padding-top: 10pt" /><br />
]

---

# Tracking *fMRIPrep*'s usage

<br />

.larger[
<i class="fa-solid fa-circle-right"></i> It's critical to report **exact versions**

<i class="fa-solid fa-circle-right"></i> Semantics inform about compatibility: **24.0.1**

<i class="fa-solid fa-circle-right"></i> Long-term support (LTS) program.

]

.center[
<img src="images/20250204_versionstream.png" style="width: 90%; padding-top: 10pt" /><br />
]

---

# Standardization in neuroimaging—impact

.boxed-content.larger.no-bullet[

* .large[<i class="fa-solid fa-hands-clapping"></i> **Not just wide adoption**]
  .indent[.gray-text[Used in large neuroscientific endeavors: ENIGMA]

  .gray-text[Industry: [Flywheel's gears for *MRIQC* and *fMRIPrep*](https://flywheel.io/insights/blog/comprehensive-support-bids#:~:text=The%20Flywheel%20Gear%20Exchange%20currently,to%20download%20your%20entire), [Amazon Web Services featured *fMRIPrep*](https://aws.amazon.com/blogs/opensource/fmri-data-preprocessing-aws-fmriprep/#:~:text=itself,efforts%20on%20preprocessing%20fMRI%20data), [QMENTA's RS fMRI tool derived from *fMRIPrep*](https://catalog.qmenta.com/tool/resting-state-fmri#:~:text=Catalog%20catalog,Networks%3B%20%E2%80%A2%20Salience%20Network)
  ]
  .gray-text[Relevant works: [Caucheteux, Gramfort & King, (2023)](https://doi.org/10.1038/s41562-022-01516-2), [Tozzi et al., (2024)](https://doi.org/10.1038/s41591-024-03057-9), [Hyon et al., (2020)](https://doi.org/10.1073/pnas.2013606117), [Yamashita et al., (2020)](https://doi.org/10.1371/journal.pbio.3000966).
  ]
  ]

* .large[<i class="fa-solid fa-people-group"></i> **Built a Lasting Community**]
  .indent[
  .gray-text[100+ researchers engaged in the community worldwide]  
  .gray-text[Community is critical to provide a footing for standardization (following BIDS' experience)]
  ]

* .large[<i class="fa-solid fa-industry"></i> **DevOps-inspired standardization**]
  .indent[
  .gray-text[Reducing the gap between research and application reversing the transference flow]
  ]
]

???

Beyond citations and the staggering usage figures I showed in my first presentation, NiPreps is a major achievement by DORA principles.

NiPreps is used in large consortia like ENIGMA, given a foundation to pioneering clinical applications such as Tozzi 2024, and is offered by major industry players like Flywheel, QMENTA, and even AWS workflows.


The NiPreps framework is driven by a community involving more than 100 scientist globally and their adoption continues growing.

That’s the real impact: it’s changing how people conduct imaging studies and ensuring higher data quality across labs.

One distinct reason for success is that we contributed to bridge the gap between research and real-world applicationi by reversing the typical knowledge transference flow:

We integrate software engineering best practices from devops and industry for the standardization and solidification of the neuroimaging workflow.

---

.center[
<img src="../assets/nipreps-chart.png" width="63%" /><br />
<em>NiPreps</em> (<a href="https://doi.org/10.31219/osf.io/ujxp6">Esteban et al., 2020</a>)
]

???

Here’s the basic architecture.

It’s an ecosystem of modular tools at different levels of abstraction, offering space for AI/ML innovation all accross individual computer vision tasks.

---
count: false

# Thank you!

.right-column3.center[
<a href="https://oesteban.github.io/talks/NEMES-GU-2025/"><object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 60%"></object></a>

Oscar Esteban &lt;<code><a href="mailto:phd@oscaresteban.es">phd@oscaresteban.es</a></code>>

**NEMES Göteborg, Sep 25<sup>th</sup>, 2025**
]

.left-column3[
.center[
<a href="https://www.axonlab.org"><img style="width: 200px;" src="../assets/axonlab_logo.png" alt="AxonLab website" /></a>
<img src="https://www.axonlab.org/images/logopic/logo_unil_chuv_fbm.png" style="width:120px; padding-left: 20px" alt="CHUV & UNIL" />

<a href="https://www.axonlab.org/team/"><img style="width: 300px" src="../assets/axonlab_people.png" /></a>

The **HCPh team** <br />
The **NiPreps Community**

<br />
<img src="../assets/snsf_logo.png" style="width:180px;" alt="SNSF" /> &nbsp;
<img src="../assets/nimh_logo.png" style="width:180px;" alt="NIMH" /> &nbsp;
<img src="../assets/czi_logo.png" style="width:100px;" alt="CZI" /> &nbsp;&nbsp;
]
]


???
</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script>
      // Use: ![:img Alt text with spaces but not commas, 50%](image.png)
      remark.macros.img = function (altText, width) {
        var url = this;
        return '<img alt="' + altText + '" src="' + url + '" style="width: ' + width + '" />';
      };
      // Use: ![:video](10.5129/10234)
      remark.macros.video = function (width) {
        var url = this;
        return '<video src="' + url + '" width="' + width + '" preload="auto" controls />';
      };
      // Use: ![:doi](10.5129/10234)
      remark.macros.doi = function () {
        var doi = this;
        return '<a href="https://doi.org/' + doi + '">' + doi + '</a>';
      };

      var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true,
          countIncrementalSlides: false,
          highlightSpans: true,
          ratio: '16:9'
      });
    </script>
  </body>
</html>
