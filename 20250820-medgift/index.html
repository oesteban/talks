<!DOCTYPE html>
<html>
  <head>
    <title>Oscar Esteban 20.08.2025</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="../assets/asciinema-player/asciinema-player.css" />
    <link rel="stylesheet" type="text/css" href="../css/hes-so.css" />
  </head>
  <body>
<script src="../assets/asciinema-player/asciinema-player.min.js"></script>
<script src="https://kit.fontawesome.com/015d4b01de.js" crossorigin="anonymous"></script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>


<textarea id="source">
name: title
layout: true
class: center
---
layout: false
count: false


.center[
<a href="https://oesteban.github.io/talks/20250820-medgift/">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 20%"></object>
  <br />
  https://oesteban.github.io/talks/20250820-medgift/
</a>

<br />
<br />

## Harnessing the reliability of neuroimaging bottom-up

Oscar Esteban &lt;<code>phd@oscaresteban.es</code>>

<br />

### Presentation at MEDGIFT's meeting — 20.08.2025
]

???

---
layout: false
count: false


.center[
<a href="https://oesteban.github.io/talks/20250820-medgift/">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 20%"></object>
  <br />
  https://oesteban.github.io/talks/20250820-medgift/
</a>

<br />
<br />

## Harnessing the reliability of neuroimaging bottom-up

Oscar Esteban &lt;<code>phd@oscaresteban.es</code>>

<br />

### Presentation at MEDGIFT's meeting — 20.08.2025
]

???

---

name: newsection
layout: true

.perma-sidebar[
<p class="rotate">
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0; height: 20px; padding-top: 6px;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
  <span style="padding-left: 10px; font-weight: 600;">Introducing Oscar Esteban | 20.08.2025</span>
</p>
]

---

# About me

.right-column3.center[
(Link to slides)
<a href="https://oesteban.github.io/talks/20250820-medgift/">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 100%"></object>
  <br />
  https://oesteban.github.io/talks/20250820-medgift/
</a>
]


.pad-top.left-column3[
.people-table.larger[

| | |
|---:|---|
| ![oscar-esteban](https://www.axonlab.org/images/teampic/OscarEsteban-300x300.jpg) | **Oscar Esteban** <br /> Associate UAS Professor, HES-SO <br /> Head of [AxonLab](https://www.axonlab.org) <br /><br /> .small[Ph.D. (2015) @ Universidad Politécnica de Madrid] <br /> .small[PD (2020) @ Stanford University] <br /> .small[Ambizione (2025) @ CHUV] |
]
<br />


.large.center[I'm a **computational neuroscientist** <br /> and **open science** advocate.]

<br />
<br />
.larger[***Mental* health—both human and machine—**is becoming the next big challenge.]

Hallucinations · Confabulation · Cognitive overload · Forgetting · Delusions · Bias · Attention · Agency · Emergent Behavior
]

---

<br />

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right"></i> .larger[**Context**]
  <br /> .indent[.gray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right"></i> .larger[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.gray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right"></i> .larger[**Standardization**]
  <br /> .indent[.gray-text[fMRIPrep & NiPreps]]

* <i class="fa-solid fa-circle-right fa-solid"></i> .larger[**Wrap-up**]
  <br /> .indent[.gray-text[Areas of interest]]
]


???

---
count:false

<br />

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right"></i> .larger[**Context**]
  <br /> .indent[.gray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Standardization**]
  <br /> .indent[.dimgray-text[fMRIPrep & NiPreps]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Wrap-up**]
  <br /> .indent[.dimgray-text[Areas of interest]]
]

???

---

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 100%; padding-top: 20pt;"></object>

.bottom-box[
.gray-text[Related publications:]
[Esteban et al., (2020)](http://doi.org/10.1038/s41596-020-0327-3);
[Niso et al., (2022)](https://doi.org/10.1016/j.neuroimage.2022.119623)
]
]

???

In broad strokes, this is the MRI-based connectivity analysis pipeline.

Interestingly, it looks like any machine learning pipeline: we acquire data, perform quality control, preprocess it, define features, and then apply statistical modeling to extract insights.

For a deeper dive into the details of the neuroimaging worflow, I've participated in several efforts such as the two references given below.

---
count:false

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-1.svg" style="width: 100%; padding-top: 20pt;"></object>

.no-bullet[
* .larger[<i class="fa-solid fa-clipboard-list"></i> Run the **MRI experiment** following Standard Operating Procedures (SOPs)]

* <i class="fa-solid fa-folder-tree"></i> .larger[**Standardized data structure** (BIDS—Brain Imaging Data Structure)]

* .larger[<i class="fa-solid fa-square-check"></i> **QA/QC** (Quality assurance / control)]

]

.bottom-box[
.gray-text[Related publications:]<br />
[Esteban et al., (2017)](https://doi.org/10.1371/journal.pone.0184661);
[Provins et al., (2023)](https://doi.org/10.3389/fnimg.2022.1073734);
[Provins et al., (2025, **RR**)](https://doi.org/10.1371/journal.pbio.3003149)
[Hagen et al., (2024, *under review*)](https://doi.org/10.1101/2024.10.21.619532)
]

]

???

The pipeline starts with data acquisition, management and quality assurance and control (QA/QC).

My early work on QA/QC applied machine learning to automatically assess MRI images of the brain, and raised awareness over issues like “site effects”—where data from different scanners or labs can systematically vary, much like “batch effects” in genomics.

One key principle in AI is that models are only as good as their data and neuroimaging is no exception.

This is why my group and I have been deeply involved in BIDS, produced version-controlled, machine-readable standard operating procedures, developed comprehensive QA/QC protocols and created ready-to-use applications to detect suboptimal data early.

---
count:false

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-2.svg" style="width: 100%; padding-top: 20pt;"></object>

.no-bullet[
* .larger[<i class="fa-solid fa-smog"></i> Detection of **nuisance sources**]

* .larger[<i class="fa-solid fa-magnifying-glass-location"></i> Spatiotemporal **location** of signals]

* .larger[<i class="fa-solid fa-location-crosshairs"></i> Definition of **brain units** of analysis (regions)]

]

.bottom-box[
.gray-text[Related publications:]
[**Esteban et al., (2019)**](https://doi.org/10.1038/s41592-018-0235-4);
[Ciric et al., (2022)](https://doi.org/10.1038/s41592-022-01681-2);
[Adebimpe et al., (2022)](https://doi.org/10.1038/s41592-022-01458-7)
]
]

???

Next in the pipeline is preprocessing, which transforms raw data into something models can reliably interpret.

For example, it identifies signals of no interest for their cleaning or accounting within modeling.

In neuroimaging, preprocessing also deals with accurately locating signals and objects in space and time, and often the definition of relevant brain regions that focus the analysis.

A decade ago, we might have called it “feature engineering.”

Regardless of the term, it’s crucial: “preprocessing” is now a keyword in job posts at major players like Anthropic, DeepMind, and OpenAI.

---

# The research workflow of connectivity analyses

.boxed-content[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-3.svg" style="width: 100%; padding-top: 20pt;"></object>

.pull-right[
<img src="../assets/matrix-fc.png" alt="matrix-fc" style="width: 20%" />
<br />
**Functional Connectivity (FC)**
<br />
.gray-text[Synchronized BOLD co-variation between brain regions]
]

.pull-left.align-right[
<img src="../assets/matrix-sc.png" alt="matrix-fc" style="width: 20%" />
<br />
**Structural Connectivity (SC)**
<br />
.gray-text[Tracked water diffusion pathways between brain regions]

]
]

.bottom-box[
.gray-text[Related publications:]
[Thompson et al., (2021, *under review*)](https://doi.org/10.1101/2021.01.16.426941);
[Rodrigue et al., (2021)](https://doi.org/10.1016/j.bpsc.2020.12.002)
]


???

Once data are ready for analysis, we formalize connectivity as undirected acyclic graphs, represented with symmetric matrices.

In the case of structural connectivity, we use diffusion MRI to track pathways of white-matter bundles connecting brain regions.

For functional connectivity we measure synchronized functional MRI signal fluctuations sensitive to the dynamic oxygen content in blood.

I’ve been involved in several works applying structural and functional connectivity.

For instance, in Rodrigue 2021, we explored ML approaches to identify clinical biomarkers.

Unfortunately, we had to conclude that, despite promising data quality and reliability remain insufficient for robust biomarker discovery.

---

# Despite active research, scarce clinical application

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 100%; padding-bottom: 10pt; padding-top: 20pt;"></object>
]

.no-bullet[
* .large[<i class="fa-solid fa-triangle-exclamation"></i> Alarming **analytical variability**:]
  * .larger[Diffusion MRI [[Maier-Hein et al., (2015)](https://doi.org/10.1038/s41467-017-01285-x)]]
  * .larger[Functional MRI [[Botvinik-Nezer et al., (2020)](https://doi.org/10.1038/s41586-020-2314-9), [Li et al., (2024)](https://doi.org/10.1038/s41562-024-01942-4)]]

<br />
* .large[<i class="fa-solid fa-dna"></i> **Like genomics** between HGP (2003) and NGS (2015)?]
]
]

???

Therefore, although simple on the surface, the workflow is highly-complex and each step introduces potential variability.

Complicating matters further, every lab around the world might use a slightly different pipeline.

Different tools, different parameters—lead to inconsistent results.

This fragmentation hinders progress and precludes between-study comparisons.

The consequence is an acute scarcity of real-world applications of connectivity analyses.

Let's compare to genomics.

Neuroimaging is today in an early stage of standardization akin to genomics at the conclusion of the Human Genome Project in 2003.

One key aspect for the remarkable advance of genomics is precisely the comprehensive standardization process of the pipeline that culminated some 10 years ago.

For that reason, I advocate for replicating this success experience in neuroscience.


---

# fMRIPrep: despite minimal, a complex workflow

.boxed-content.center[
<img src="https://github.com/oesteban/fmriprep/raw/f4c7a9804be26c912b24ef4dccba54bdd72fa1fd/docs/_static/fmriprep-21.0.0.svg" style="width: 100%; padding-top: 20pt" />
]

???

Here I'm showing a very much simplified overview of fMRIPrep's design.

Despite it being just "minimal" preprocessing and just one step of the pipeline, its complexity is evident.

Rather than delving into each of these steps, I'll just say that there's a current need to update and improve computer vision algorithms and approaches (segmentation, image registration, dimensionality reduction, etc.) with AI/ML all across the dataflow.

---

# Despite active research, no real-world application

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 100%; padding-bottom: 10pt; padding-top: 20pt;"></object>
]

.no-bullet[
* .large[<i class="fa-solid fa-triangle-exclamation"></i> Alarming **analytical variability**:]
  * .larger[Diffusion MRI [[Maier-Hein et al., (2015)](https://doi.org/10.1038/s41467-017-01285-x)]]
  * .larger[Functional MRI [[Botvinik-Nezer et al., (2020)](https://doi.org/10.1038/s41586-020-2314-9), [Li et al., (2024)](https://doi.org/10.1038/s41562-024-01942-4)]]

<br />
* .large[<i class="fa-solid fa-magnifying-glass-plus"></i> Insufficient **test-retest reliability** at the finer scales of brain units]
]
]

???

On top of all that methodological variability, there’s also inherent unreliability in measuring connectivity from session to session.

If you scan the same participant twice, back-to-back, you can still get different connectivity measurements, indicating limited test-retest reliability.

Despite all these concerns, more than 20 thousand papers were published focusing on functional connectivity and 10 thousand on structural connectivity in 2024, as per Pubmed.

---

# More reliable measurements

.center[
.boxed-content[
<object type="image/svg+xml" data="../assets/reproducible-definition-grid-axes-2.svg" style="width: 65%;"></object>

[Esteban, (2025)](https://doi.org/10.1007/978-1-0716-4260-3_8) [[Methods for analyzing large neuroimaging datasets](https://doi.org/10.1007/978-1-0716-4260-3)]
]
]

---
count:false

<br />

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  <br /> .indent[.dimgray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right"></i> .larger[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.gray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Standardization**]
  <br /> .indent[.dimgray-text[fMRIPrep & NiPreps]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Wrap-up**]
  <br /> .indent[.dimgray-text[Areas of interest]]
]

???

Having deeply focused on the standardization to address analytical variability, in 2020 I started to work on the reliability problem of connectivity analysis.

Tat that moment, I identified that the neuroimaging community was lacking an ultra-dense, calibration-grade dataset to address the problem.

With the support of an Ambizione project, I launched the Human Connectome Phantom project.

The idea is to have a single subject—myself—scanned intensively within a very short time window to minimize brain changes (and hence, serve as a "phantom" reference).

The dataset needed to combine sophisticated functional and structural MR acquisitions, and a wealth of physiological tracking.

---

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Avoid **between-subject** variability.

Short span of time to minimize brain changes (~8 weeks)

* Wave 1: **within-scanner variability**
* Wave 2: **between-scanner variability**

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-00.svg" style="width: 90%;"></object>
]

???

As mentioned, I lent myself as the phantom given the hefty commitment the project involved and the total absence of antecedents of a human being scanned so frequently.


---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
**Anatomy**:
  * T<sub>1</sub>-weighted & T<sub>2</sub>-weighted

**Diffusion MRI** (HARDI):

  * 1.6 × 1.6 × 1.6 [mm<sup>3</sup>]
  * 6 × b = 0 s/mm<sup>2</sup>
  * 274 DWIs <br />137 DWIs non-collinear
    b={700,1000,2000,3000} s/mm<sup>2</sup>

**Functional MRI** (BOLD): <br />(next slide)

***B<sub>0</sub>* fieldmaps**:

  * GRE (2min 38s)
  * 4 × EPI/dMRI (54s)
  * 4 × EPI/fMRI (27s)
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-01.svg" style="width: 90%;"></object>
]

???

First, our study focused on a single scanner at CHUV.

We collected a very comprehensive MRI protocol with technical sophistication to provide a solid foundation to secondary methodological developments.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
**Functional MRI**:

  * 2.2 × 2.2 × 2.2 [mm<sup>3</sup>]
  * TR = 1.6s
  * multi-echo (4 echos)
  * three tasks:
    * QC task (3min 7s)
    * naturalistic movie resting (20min 9s)
    * breath-holding task (6min 0s)

<video width="100%" autoplay loop muted>
  <source src="../assets/mundaka-clip.webm" type="video/webm">
  Your browser does not support the video tag.
</video>

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-03.svg" style="width: 90%;"></object>
]

???
We collected eye-tracking to assess arousal and analyze correlation with other facets such as head motion.
We also obtained a wealth of signals to investigate physiological variability, including expired oxygen and CO2 (which is far from standard practice but needed).

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Collected within **4 weeks**

Following stringent **Standard Operating Procedures (SOPs)**

.center[
<a href="https://www.axonlab.org/hcph-sops">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 80%"></object>
  <br />
  Link to **public** SOPs
</a>
]

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-04.svg" style="width: 90%;"></object>
]

???

This first wave of data was collected in about 3 weeks to ensure minimal changes on the human brain phantom.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
<a href="https://www.axonlab.org/hcph-sops/data-management/intro/">Data Management (SOPs)</a>

* BIDS Conversion & Validation

* QA/QC with *MRIQC*

* DataLad: version control

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-05.svg" style="width: 90%;"></object>
]

???

Data were then standardized, properly stored under version control, and quality-controlled.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
<a href="https://www.axonlab.org/hcph-sops/processing/preprocessing/">Preprocessing (SOPs)</a> with *fMRIPrep* and *dMRIPrep*

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-06.svg" style="width: 90%;"></object>
]

???

We preprocessed the data with NiPreps

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
<a href="https://www.axonlab.org/hcph-sops/processing/functional-connectivity/">Connectivity extraction (SOPs)</a> (mostly *NiLearn*)
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-07.svg" style="width: 90%;"></object>
]

???

Next comes connectivity extraction.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
We collected 36 sessions
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-08.svg" style="width: 90%;"></object>
]

???

And this process yielded at least 36 pairs of structural and functional matrices.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Objective: **testing generalization**

* Clinical-use scanners

* Homogenized protocol

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-09.svg" style="width: 90%;"></object>
]

???

But we also wanted to test agreement between scanners.

To do so, we modified the protocol so that it would be suitable for the two clinical scanners with more limited performance.

We collected 12 more sessions on the research scanner with the homogenized protocol.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
We collected 36 sessions: 12 sessions each scanner (x3)
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-10.svg" style="width: 90%;"></object>
]

???

And 12 more sessions per scanner on the two clinical scanners.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Data will be released fully open (CC-BY), including derivatives.

]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-full.svg" style="width: 90%;"></object>
]

???

Finally, we committed to release all materials (data, code, operating procedures, etc.) open.

---

## HCPh: Registered-report driven

.right-column3[
.larger.center[
.large[**Stage 1 RR**] <br />
Nature Methods

<a href="https://doi.org/10.17605/OSF.IO/VAMQ6"><object type="image/svg+xml" data="../ISMRM2024/images/qr-RR.svg" style="width: 85%"></object>
Provins et al., (2023)</a>
]
]

.left-column3[
<object type="image/svg+xml" data="../assets/hcph-full.svg" style="width: 90%;"></object>
]

???

The study was accepted in October 2023 as a Stage 1 Registered Report at Nature Methods.

The registered report, data collection, and the research was carried out by a large team, with the outstanding role of the Ph.D. student funded by the Ambizione (Celine Provins), and is accessible with the QR code.

By pre-registering, we locked in our hypotheses and methods up front and addressed design issues raised by three reviewers before data collection started.

---

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/neuroimaging-workflow-large.svg" style="width: 90%; padding-bottom: 55.5pt; padding-top: 120pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-00.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-01.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-02.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-03.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-04.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol-05.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---
count:false

# QA/QC of the neuroimaging worflow

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol.svg" style="width: 100%; padding-bottom: 5pt; padding-top: 40pt;"></object>
]

.align-right[
([Provins et al., 2023](http://doi.org/10.3389/fnimg.2022.1073734))
]
]

???

---

# 'Swiss-cheese security model' for QC

.boxed-content[
.center[
<object type="image/svg+xml" data="../assets/provins-qaqc-protocol.svg" style="width: 60%; padding-bottom: 10pt; padding-top: 10pt;"></object>

<object type="image/svg+xml" data="../ISMRM2024/images/swiss-cheese.svg" style="width:60%"></object>

.small[
[BenAveling @ wikipedia](https://en.wikipedia.org/wiki/Swiss_cheese_model#/media/File:Swiss_cheese_model_textless.svg)
]
]

]

???

For those with knowledge about security protocols, this approach will surely evoke the Swiss cheese model.

The model assumes that all QC checkpoints will have holes through which data progresses toward analysis.

By layering several QC checkpoints looking at the data in different ways, we make sure that images with potential to bias the results do not make all the way through the workflow.

---

# HCPh: Study Outcomes


.boxed-content.no-bullet.pad-top[

* .large.pad-top[<i class="fa-solid fa-database"></i> One-of-a-kind, open **dataset**]
  .indent[
  .gray-text[Thoroughly documented, with open [Standard Operating Procedures (SOPs)](https://www.axonlab.org/hcph-sops)]
  ]

* .large[<i class="fa-solid fa-chart-bar"></i> Within-scanner **reliability characterization**]
  .indent[
  .gray-text[PCA-based modeling of FC residuals]

  .gray-text[Bayesian modeling of SC]
  ]

* .large[<i class="fa-solid fa-scale-balanced"></i> Between-scanner **agreement**]
  .indent[
  .gray-text[One research scanner vs. two clinical scanners]
  ]

]

???

We are currently concluding the final Stage 2 reporting the results, to be submitted to the journal within the next 3-4 months.

We have collected and preprocessed a unique dataset and publicly released its comprehensive standard operating procedures.

Following our pre-registered commitments, we are currently executing our promised analysis involving principal components analyses of functional connectivity and bayesian modelling of structural connectivity.

Finally, we will investigate how structural and functional connectivity measurements agree if obtained with different devices.

This will yield a baseline measure of how stable connectivity metrics can be if the protocol is maximally controlled.

---

# HCPh: Significance
<br />

.boxed-content.no-bullet[

* .large.pad-top[<i class="fa-solid fa-ruler"></i> Complement standardization with **calibration**]
  .indent[
  .gray-text[Provide normative distributions of SC and FC (prior knowledge)]

  .gray-text[An approach to design *calibrated* studies, potentially with the same or new **human phantoms**]
  ]

* .large[<i class="fa-solid fa-route"></i> Methodological **advance**]
  .indent[
  .gray-text[Removal of physiological variability, artifacts, etc.]

  .gray-text[Between-scanner agreement of measurements (other than connectivity)]
  ]

* .large[<i class="fa-solid fa-building-columns"></i> A **foundational dataset**]
  .indent[
  .gray-text[SC & FC training dataset]

  .gray-text[Fully-open, ideal for courses and training]
  ]

]

???

But the impact of HCPh goes far beyond the analyses committed within the Stage 1 report.

We hope to see new “human phantom” datasets at multiple sites, enabling direct cross-site calibration.

HCPh can also serve as ground truth data for AI-driven motion-correction algorithms, generative modeling, or even real-time scanning protocols.

Essentially, we’re complementing standardization efforts (like NiPreps) with a calibration resource that addresses test-retest and multi-site reproducibility.

---
count:false

<br />
.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  <br /> .indent[.dimgray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right fa-solid"></i> .larger[**Standardization**]
  <br /> .indent[.gray-text[fMRIPrep & NiPreps]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Wrap-up**]
  <br /> .indent[.dimgray-text[Areas of interest]]
]

???

---

# Standardization in neuroimaging—NiPreps

.boxed-content[
<br />
.larger.center[
"*analysis-grade*" data <i class="fa-solid fa-circle-right"></i> data **directly consumable by analyses**
]

.pull-left[
<br />
<br />
*Analysis-grade* data is an analogy to the concept of "*sushi-grade (or [sashimi-grade](https://en.wikipedia.org/wiki/Sashimi)) fish*" in that both are:

.large[<i class="fa-solid fa-circle-right"></i> **minimally preprocessed**,]

and

.large[<i class="fa-solid fa-circle-right"></i> **safe to consume** directly.]
]

.pull-right.center[
<img src="../assets/nipreps-transparent.png" width="100%" />
<a href="https://www.nipreps.org"><object type="text/xml+svg" data="../assets/qr-nipreps-url.svg" style="width: 40%"></object><br />
www.nipreps.org</a>
]
]

???

In computational neuroscience I'm recognized by my standardization efforts.

NiPreps is a framework for standardized preprocessing pipelines in neuroimaging.

I often use the “sushi-grade” analogy

just like sushi-grade fish is minimally processed but safe to consume,

NiPreps yields “analysis-grade” data, with minimal processing interventions but ready for direct machine learning ingestion.

---

# Tracking *fMRIPrep*'s adoption

.center[
<img src="https://www.nipreps.org/assets/20250811_weekly.png" style="width: 90%; padding-top: 10pt" /><br />
]

---

# Tracking *fMRIPrep*'s usage

<br />

.larger[
<i class="fa-solid fa-circle-right"></i> It's critical to report **exact versions**

<i class="fa-solid fa-circle-right"></i> Semantics inform about compatibility: **24.0.1**

<i class="fa-solid fa-circle-right"></i> Long-term support (LTS) program.

]

.center[
<img src="../20250410-research/images/20250204_versionstream.png" style="width: 90%; padding-top: 10pt" /><br />
]

---

# Standardization in neuroimaging—impact

.boxed-content.larger.no-bullet[

* .large[<i class="fa-solid fa-hands-clapping"></i> **Not just citations & wide adoption**]
  .indent[.gray-text[Used in large neuroscientific endeavors: ENIGMA]

  .gray-text[Industry: [Flywheel's gears for *MRIQC* and *fMRIPrep*](https://flywheel.io/insights/blog/comprehensive-support-bids#:~:text=The%20Flywheel%20Gear%20Exchange%20currently,to%20download%20your%20entire), [Amazon Web Services featured *fMRIPrep*](https://aws.amazon.com/blogs/opensource/fmri-data-preprocessing-aws-fmriprep/#:~:text=itself,efforts%20on%20preprocessing%20fMRI%20data), [QMENTA's RS fMRI tool derived from *fMRIPrep*](https://catalog.qmenta.com/tool/resting-state-fmri#:~:text=Catalog%20catalog,Networks%3B%20%E2%80%A2%20Salience%20Network)
  ]
  .gray-text[Relevant works: [Tozzi et al., (2024)](https://doi.org/10.1038/s41591-024-03057-9), [Caucheteux, Gramfort & King, (2023)](https://doi.org/10.1038/s41562-022-01516-2), [Hyon et al., (2020)](https://doi.org/10.1073/pnas.2013606117), [Yamashita et al., (2020)](https://doi.org/10.1371/journal.pbio.3000966).
  ]
  ]

* .large[<i class="fa-solid fa-people-group"></i> **Built a Lasting Community**]
  .indent[
  .gray-text[100+ researchers engaged in the community worldwide]  
  .gray-text[Community is critical to provide a footing for standardization (following BIDS' experience)]
  ]

* .large[<i class="fa-solid fa-industry"></i> **DevOps-inspired standardization**]
  .indent[
  .gray-text[Reducing the gap between research and application reversing the transference flow]
  ]
]

???

Beyond citations and the staggering usage figures I showed in my first presentation, NiPreps is a major achievement by DORA principles.

NiPreps is used in large consortia like ENIGMA, given a foundation to pioneering clinical applications such as Tozzi 2024, and is offered by major industry players like Flywheel, QMENTA, and even AWS workflows.


The NiPreps framework is driven by a community involving more than 100 scientist globally and their adoption continues growing.

That’s the real impact: it’s changing how people conduct imaging studies and ensuring higher data quality across labs.

One distinct reason for success is that we contributed to bridge the gap between research and real-world applicationi by reversing the typical knowledge transference flow:

We integrate software engineering best practices from devops and industry for the standardization and solidification of the neuroimaging workflow.

---

.center[
<img src="../assets/nipreps-chart.png" width="63%" /><br />
<em>NiPreps</em> (<a href="https://doi.org/10.31219/osf.io/ujxp6">Esteban et al., 2020</a>)
]

???

Here’s the basic architecture.

It’s an ecosystem of modular tools at different levels of abstraction, offering space for AI/ML innovation all accross individual computer vision tasks.

---
count:false

<br />
.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  <br /> .indent[.dimgray-text[Structural and functional connectivity of the brain]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  <br /> .indent[.dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]]

* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Standardization**]
  <br /> .indent[.dimgray-text[fMRIPrep & NiPreps]]

* <i class="fa-solid fa-circle-right fa-solid"></i> .larger[**Wrap-up**]
  <br /> .indent[.gray-text[Areas of interest]]
]

???

---
.boxed-content.centered[
<object type="image/svg+xml" data="../20250410-research/images/venn-interests.svg" style="padding-top: 40px; width: 99%"></object>
]

???

This diagram shows the intersection of my core areas: neuroimaging and computational neuroscience, ML/AI and computer vision, and finally Software engineering & standardization (S&S).

I have identified three main funding opportunities which can be loosely assigned to each of these areas.



---
layout: false
count: false


.center[
<a href="https://oesteban.github.io/talks/20250820-medgift/">
  <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 20%"></object>
  <br />
  https://oesteban.github.io/talks/20250820-medgift/
</a>

<br />
<br />

# Thank you

Oscar Esteban &lt;<code>phd@oscaresteban.es</code>>

]

???

</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script>
      // Use: ![:img Alt text with spaces but not commas, 50%](image.png)
      remark.macros.img = function (altText, width) {
        var url = this;
        return '<img alt="' + altText + '" src="' + url + '" style="width: ' + width + '" />';
      };
      // Use: ![:video](10.5129/10234)
      remark.macros.video = function (width) {
        var url = this;
        return '<video src="' + url + '" width="' + width + '" preload="auto" controls />';
      };
      // Use: ![:doi](10.5129/10234)
      remark.macros.doi = function () {
        var doi = this;
        return '<a href="https://doi.org/' + doi + '">' + doi + '</a>';
      };

      var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true,
          countIncrementalSlides: false,
          highlightSpans: true,
          ratio: '16:9'
      });
    </script>
  </body>
</html>
