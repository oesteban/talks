<!DOCTYPE html>
<html>
  <head>
    <title>Oscar Esteban 06.03.2025</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/talks/assets/asciinema-player/asciinema-player.css" />
    <style>
      @import url(https://fonts.googleapis.com/css?family=Roboto+Mono:400,700,400italic);

      .blur {
          -webkit-filter: blur(5px) opacity(.3);
          -moz-filter: blur(10px) opacity(.3);
          -o-filter: blur(5px) opacity(.3);
          -ms-filter: blur(5px) opacity(.3);
          filter: blur(5px) opacity(.3);
      }

      html {
        height: 100%;
      }
      body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        height: 100%;
      }
      h1, h2, h3 {
        font-weight: 600;
        margin-bottom: 0;
      }

      .middle {
        margin: 0;
        position: absolute;
        top: 50%;
        width:100%;
        -ms-transform: translateY(-50%);
        transform: translateY(-50%);
      }

      .remark-slide-content { height: 100%; padding: 0; font-size: 16pt;}
      .remark-slide-content h1 { font-size: 2em; color: #738373; }
      .remark-slide-content h2 { font-size: 1.5em; color: #738373; }
      .remark-slide-content h3 { font-size: 1.2em; color: #738373; }
      .footnote {
        position: absolute;
        bottom: 0.5em;
        font-size: 0.7em;
  left: 6em;
      }
      li p { line-height: 1.25em; }

      .remark-slide-content>p { margin-left: 60px; }
      .remark-slide-content>ul { margin-left: 60px; }
      .remark-slide-content>ul li { margin-left: 0.8em; }

      .no-bullet > ul {
        list-style-type: none;
        padding-left:  0;
      }

      .no-bullet > ul > li > ul {
        padding-left:  2.8em;
      }

      .red { color: #fa0000; }
      .blue { color: #0000fa; }
      .green { color: #698b69; }

      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #e7e8e2;
        border-radius: 3px;
      }
      .remark-code, .remark-inline-code { font-family: 'Roboto Mono'; }
      .large .remark-code, .large .remark-inline-code { font-size: 0.8em; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 45%;
      }
      .pull-right {
        float: right;
        width: 45%;
        height: 75%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .section-separator .middle {
        margin-left: 210px;
        width: 80%;
      }

      .fa-solid {
        color: #026c00;
      }
      .fa-brands {
        color: #026c00;
      }
      .fa-blank {
  color: #ffffff;
      }

      .fa-solid-dim {
        color: #026c0020;
      }

      em {
        color: #026c00;
      }

      strong {
        color: #026c00;
      }

      a { color: #026c00; text-decoration: underline}

      .gray-text {
        color: #888;
      }
      .gray-text em, .gray-text strong {
        color: #888;
      }

      .dim strong {
        color: #026c0020;
      }

      .dim em {
        color: #026c0020;
      }

      .dimgray-text {
        color: #08080820;
      }

      .perma-sidebar {
        float: left;
        background-color: #009933;
        color: #f4f4f4;
        width: 40px;
        height: 100%;
        padding: 0;
        margin: 0 2em 0 0;
        text-align: center;
      }
      .perma-sidebar p {
        text-align: left;
        font-size: 80%;
        height: 35px;
        width: 670px;
        margin: 320px 0 0 -315px;
      }
      .perma-sidebar h2:last-of-type, .perma-sidebar h3:last-child {
        color: #d2c295;
      }

/*      .sidebar-slug {
          bottom: 12px;
          left: 0;
          position: absolute;
          width: 210px;
          text-align: center;
      }
      .sidebar-slug img {
          width: 180px;
      }*/

      .svg-reportlet { width: 75%; }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        /*font-size: .9em;*/
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        width: 23%;
        height: 82%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 65%;
        float: right;
        padding-top: 1em;
      }
      /* Two-column layout (40% left) */
      .left-column2 {
        width: 35%;
        height: 85%;
        float: left;
      }
        .left-column2 h2:last-of-type, .left-column2 h3:last-child {
          color: #000;
        }
      .right-column2 {
        width: 50%;
        float: right;
        padding-top: 1em;
        margin-right: 2.5em;
      }
      /* Two-column layout (60% left) */
      .left-column3 {
        display: block;
        width: auto;
        height: 85%;
        margin: 0 32% 0 5%;
      }
        .left-column3 h2:last-of-type, .left-column3 h3:last-child {
          color: #000;
        }

      .left-column3 li {
        margin-left: 25px;
      }

      .right-column3 {
        width: 30%;
        float: right;
        padding-top: 1em;
        padding-right: 1em;
      }
      /* Two-column layout (even split) */
      .left-column-mid {
        width: 45%;
        float: left;
      }
      .right-column-mid {
        width: 45%;
        float: right;
      }
      /* Two-column layout (flipped) */
      .left-column-inv {
        color: #777;
        width: 75%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column-inv {
        width: 20%;
        float: right;
        padding-top: 1em;
      }
      .caption {
          font-size: 0.7em;
      }
      .slide-slug {
          bottom: 12px;
          opacity: .5;
          position: absolute;
          left: 4em;
      }

      .small code {
        font-size: 9pt;
      }
      .tiny code {
        font-size: 8.5pt;
      }

      .small {
        font-size: 0.7em;
      }

      .larger {
        font-size: 1.1em;
      }

      .large {
        font-size: 1.6em;
      }

      .boxed-content {
        float: left;
        display: block;
        width: 89%;
        padding-right: 4%;
      }

      .boxed-bottom {
        clear: both;
        margin-top: 0px;
      }
/*
      .distribute {
        display: flex;
        justify-content: space-between;
        flex-direction: column;
        height: 100%;
        width: 100%;
      }*/

      .distribute {
        display: flex;
        justify-content: space-between;
        flex-direction: column;
        height: 80%;
        width: 100%;
      }

      .cut-right {
        margin-right: 100px;
      }

      .align-right {
        text-align: right;
      }

      .rotate{
        -webkit-transform: rotate(-90deg);
        -moz-transform: rotate(-90deg);
        -o-transform: rotate(-90deg);
        -ms-transform: rotate(-90deg);
        transform: rotate(-90deg);
      }

      .hidden-text {
        color: transparent; /* make the text invisible */
        user-select: none; /* prevent selection of the text */
      }

      .pad-left {
        padding-left: 2.0em;
      }
      .install-cmd {
          top: 20px;
          position: absolute;
          right: 4em;
      }
      figure {
        display: block;
        margin-left: auto;
        margin-right: auto;
        font-size: 8pt;
      }
      figcaption {
        text-align: right;
        font-size: 1.2em;
      }

      .program-table td:nth-child(2) {
        font-size: 1.2em;
        
      }
      .program-table td:nth-child(1) {
        font-size: 0.9em;
        color: #999;
        padding-right: 20px;
      }

      .people-table td:nth-child(1) {
        padding-right: 20px;
      }
      .people-table img {
        object-fit: cover;
        width: 200px;
        height: 200px;
        filter: grayscale(1);
      }
      .indent {
        padding-left: 50px;
      }

      .many-people-table td:nth-child(1) {
        padding-right: 20px;
      }
      .many-people-table td:nth-child(1) img {
        object-fit: cover;
        width: 80px;
        height: 80px;
      }

      .top-right {
        font-size: 0.8em;
        position:  absolute;
        width: 100px;
        text-align: center;
        padding: 0;
        margin: 0;
        top: 0;
        right: 5%;
      }

      .pad-top li {
        padding-top: 1.1em;
      }

      div.vertical-center {
        /*left: 0;*/
        width:  85%;
        margin: -100px 20px 40px 100px;
        position: absolute;
        text-align: center;
        top: 50%;
      }

      div.white-bg {
        rgba(0, 0, 0, 0.6);
      }
    </style>
  </head>
  <body>
<script src="/talks/assets/asciinema-player/asciinema-player.min.js"></script>
<script src="https://kit.fontawesome.com/015d4b01de.js" crossorigin="anonymous"></script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>


<textarea id="source">

name: title
layout: true
class: center
---
layout: false
count: false


.center[
<a href="https://oesteban.github.io/talks/20250306/">
  <img src="images/qr-talk-url.svg" alt="workflow" style="width: 20%" />
  <br />
  https://oesteban.github.io/talks/20250306/
</a>

<br />
<br />

## Harnessing the reliability of neuroimaging bottom-up

Oscar Esteban &lt;<code>phd@oscaresteban.es</code>>

<br />

### Presentation at Idiap Research Institute — 06.03.2025
]

???

---
layout: false
count: false


.center[
<a href="https://oesteban.github.io/talks/20250306/">
  <img src="images/qr-talk-url.svg" alt="workflow" style="width: 20%" />
  <br />
  https://oesteban.github.io/talks/20250306/
</a>

<br />
<br />

## Harnessing the reliability of neuroimaging bottom-up

Oscar Esteban &lt;<code>phd@oscaresteban.es</code>>

<br />

### Presentation at Idiap Research Institute — 06.03.2025
]

???

Hello again, everyone!

My name is Oscar Esteban.

Welcome to my second interview for the Research Scientist position at Idiap.

I'm excited to be back to share more details about my work and how it aligns with Idiap and the position.

---

name: newsection
layout: true

.perma-sidebar[
<p class="rotate">
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0; height: 20px; padding-top: 6px;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
  <span style="padding-left: 10px; font-weight: 600;">Introducing Oscar Esteban | 06.03.2025</span>
</p>
]

---

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right"></i> .larger[**Context**]
  .indent[
  .gray-text[Structural and functional connectivity of the brain]
  ]
* <i class="fa-solid fa-circle-right"></i> .larger[**The Human Connectome PHantom (HCPh)**]
  .indent[
  .gray-text[A New Standard for Dense, Multi-Modal Neuroimaging]
  ]
* <i class="fa-solid fa-circle-right"></i> .larger[**Major achievements**]
  .indent[
  .gray-text[Standardization in neuroimaging]
  
  .gray-text[Open Science advocacy, training, and mentorship]
  ]
* <i class="fa-solid fa-circle-right"></i> .larger[**Grant timeline**]
]

???

As requested, I’ll follow the structure provided for this second interview:

I’ll discuss one major scientific work—my Human Connectome Phantom or HCPh project—

then highlight two achievements,

and finally share my grant submission plan.

But first, I will allow myself to include a bit of context along the way to situate my research program and future goals.


---
count:false

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right"></i> .larger[**Context**]
  .indent[
  .gray-text[Structural and functional connectivity of the brain]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  .indent[
  .dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Major achievements**]
  .indent[
  .dimgray-text[Standardization in neuroimaging]
  
  .dimgray-text[Open Science advocacy, training, and mentorship]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Grant timeline**]
]

???

I work in neuroimaging, specifically on structural and functional connectivity, which involves mapping how different parts of the brain interconnect and communicate.

In many ways, these analyses are reminiscent of graph-related machine learning problems—nodes, edges, and so forth—

---

# The research workflow of connectivity analyses

.boxed-content[
<img src="../../../talks/assets/neuroimaging-workflow-large.svg" alt="workflow" style="width: 100%" />


.center[
[Esteban et al., (2020)](http://doi.org/10.1038/s41596-020-0327-3);
[Niso et al., (2022)](https://doi.org/10.1016/j.neuroimage.2022.119623)
]
]

???

In broad strokes, this is the MRI-based connectivity analysis pipeline.

Interestingly, it looks like any machine learning pipeline: we acquire data, perform quality control, preprocess it, define features, and then apply statistical modeling to extract insights.

For a deeper dive into the details of the neuroimaging worflow, I've participated in several efforts such as the two references given below.

---
count:false

# The research workflow of connectivity analyses

.boxed-content[
<img src="../../../talks/assets/neuroimaging-workflow-1.svg" alt="workflow" style="width: 100%" />


.center[
[Esteban et al., (2017)](https://doi.org/10.1371/journal.pone.0184661);
[Provins et al., (2023)](https://doi.org/10.3389/fnimg.2022.1073734); <br />
[Provins et al., (2024, *Stage 2 RR under review*)](https://doi.org/10.1101/2024.10.11.617777)
[Hagen et al., (2024, *under review*)](https://doi.org/10.1101/2024.10.21.619532)
]
<br />

.no-bullet[
* .larger[<i class="fa-solid fa-clipboard-list"></i> Run the **MRI experiment** following Standard Operating Procedures (SOPs)]

* <i class="fa-solid fa-folder-tree"></i> .larger[**Standardized data structure** (BIDS—Brain Imaging Data Structure)]

* .larger[<i class="fa-solid fa-square-check"></i> **QA/QC** (Quality assurance / control)]

]

]

???

The pipeline starts with data acquisition, management and quality assurance and control (QA/QC).

My early work on QA/QC applied machine learning to automatically assess MRI images of the brain, and raised awareness over issues like “site effects”—where data from different scanners or labs can systematically vary, much like “batch effects” in genomics.

One key principle in AI is that models are only as good as their data and neuroimaging is no exception.

This is why my group and I have been deeply involved in BIDS, produced version-controlled, machine-readable standard operating procedures, developed comprehensive QA/QC protocols and created ready-to-use applications to detect suboptimal data early.

---
count:false

# The research workflow of connectivity analyses

.boxed-content[
<img src="../../../talks/assets/neuroimaging-workflow-2.svg" alt="workflow" style="width: 100%" />


.center[
[Esteban et al., (2019)](https://doi.org/10.1038/s41592-018-0235-4);
[Ciric et al., (2022)](https://doi.org/10.1038/s41592-022-01681-2);
[Adebimpe et al., (2022)](https://doi.org/10.1038/s41592-022-01458-7)
]
<br />

.no-bullet[
* .larger[<i class="fa-solid fa-smog"></i> Detection of **nuisance sources**]

* .larger[<i class="fa-solid fa-magnifying-glass-location"></i> Spatiotemporal **location** of signals]

* .larger[<i class="fa-solid fa-location-crosshairs"></i> Definition of **brain units** of analysis (regions)]

]
]

???

Next in the pipeline is preprocessing, which transforms raw data into something models can reliably interpret.

For example, it identifies signals of no interest for their cleaning or accounting within modeling.

In neuroimaging, preprocessing also deals with accurately locating signals and objects in space and time, and often the definition of relevant brain regions that focus the analysis.

A decade ago, we might have called it “feature engineering.”

Regardless of the term, it’s crucial: “preprocessing” is now a keyword in job posts at major players like Anthropic, DeepMind, and OpenAI.

---

# fMRIPrep: despite minimal, a complex workflow

.boxed-content.center[
<br />
<br />
<img src="https://github.com/oesteban/fmriprep/raw/f4c7a9804be26c912b24ef4dccba54bdd72fa1fd/docs/_static/fmriprep-21.0.0.svg" width="100%" />
]

???

Here I'm showing a very much simplified overview of fMRIPrep's design.

Despite it being just "minimal" preprocessing and just one step of the pipeline, its complexity is evident.

Rather than delving into each of these steps, I'll just say that there's a current need to update and improve computer vision algorithms and approaches (segmentation, image registration, dimensionality reduction, etc.) with AI/ML all across the dataflow.

---

# The research workflow of connectivity analyses

.boxed-content[
<img src="../../../talks/assets/neuroimaging-workflow-3.svg" alt="workflow" style="width: 100%" />


.center[
[Thompson et al., (2021, *under review*)](https://doi.org/10.1101/2021.01.16.426941);
[Rodrigue et al., (2021)](https://doi.org/10.1016/j.bpsc.2020.12.002);
[Li et al., (2024)](https://doi.org/10.1038/s41562-024-01942-4)
]
]

.pull-right[
<img src="../../../talks/assets/matrix-fc.png" alt="matrix-fc" style="width: 20%" />
<br />
**Functional Connectivity (FC)**
<br />
.gray-text[Synchronized BOLD co-variation between brain regions]
]

.pull-left.align-right[
<img src="../../../talks/assets/matrix-sc.png" alt="matrix-fc" style="width: 20%" />
<br />
**Structural Connectivity (SC)**
<br />
.gray-text[Tracked water diffusion pathways between brain regions]

]

???

Once data are ready for analysis, we formalize connectivity as undirected acyclic graphs, represented with symmetric matrices.

In the case of structural connectivity, we use diffusion MRI to track pathways of white-matter bundles connecting brain regions.

For functional connectivity we measure synchronized functional MRI signal fluctuations sensitive to the dynamic oxygen content in blood.

I’ve been involved in several works applying structural and functional connectivity.

For instance, in Rodrigue 2021, we explored ML approaches to identify clinical biomarkers.

Unfortunately, we had to conclude that, despite promising data quality and reliability remain insufficient for robust biomarker discovery.

---

# Despite active research, no real-world application

.boxed-content[
.center[
<img src="../../../talks/assets/neuroimaging-workflow-large.svg" alt="workflow" style="width: 50%" />
]

<br />

.no-bullet[
* .large[<i class="fa-solid fa-triangle-exclamation"></i> Alarming **analytical variability**:]
  * .larger[Structural MRI [[Maier-Hein et al., (2017)](https://doi.org/10.1038/s41467-017-01285-x)]] - w/ J.-P. Thiran
  * .larger[Functional MRI [[Botvinik-Nezer et al., (2020)](https://doi.org/10.1038/s41586-020-2314-9)]] - w/ R. Poldrack

* .large[<i class="fa-solid fa-handshake-simple"></i> Incipient **standardization**] .larger[[[Esteban, (2025)](https://doi.org/10.1007/978-1-0716-4260-3_8)]]
]
]

???

Therefore, although simple on the surface, the workflow is highly-complex and each step introduces potential variability.

Complicating matters further, every lab around the world might use a slightly different pipeline.

Different tools, different parameters—lead to inconsistent results.

This fragmentation hinders progress and precludes between-study comparisons.

The consequence is an acute scarcity of real-world applications of connectivity analyses.

Let's compare to genomics.

Neuroimaging is today in an early stage of standardization akin to genomics at the conclusion of the Human Genome Project in 2003.

One key aspect for the remarkable advance of genomics is precisely the comprehensive standardization process of the pipeline that culminated some 10 years ago.

For that reason, I advocate for replicating this success experience in neuroscience.

---
count: false

# Despite active research, no real-world application

.boxed-content[
.center[
<img src="../../../talks/assets/neuroimaging-workflow-large.svg" alt="workflow" style="width: 50%" />
]

<br />

.no-bullet[
* .large[<i class="fa-solid fa-triangle-exclamation"></i> Alarming **analytical variability**:]
  * .larger[Structural MRI [[Maier-Hein et al., (2017)](https://doi.org/10.1038/s41467-017-01285-x)]] - w/ J.-P. Thiran
  * .larger[Functional MRI [[Botvinik-Nezer et al., (2020)](https://doi.org/10.1038/s41586-020-2314-9)]] - w/ R. Poldrack

* .large[<i class="fa-solid fa-handshake-simple"></i> Incipient **standardization**] .larger[[[Esteban, (2025)](https://doi.org/10.1007/978-1-0716-4260-3_8)]]

<br />
* .large[<i class="fa-solid fa-triangle-exclamation"></i> Notorious **test-retest unreliability**]
]
]

???

On top of all that methodological variability, there’s also inherent unreliability in measuring connectivity from session to session.

If you scan the same participant twice, back-to-back, you can still get different connectivity measurements, indicating limited test-retest reliability.

Despite all these concerns, more than 20 thousand papers were published focusing on functional connectivity and 10 thousand on structural connectivity in 2024, as per Pubmed.

---
count:false

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  .indent[
  .dimgray-text[Structural and functional connectivity of the brain]
  ]
* <i class="fa-solid fa-circle-right"></i> .larger[**The Human Connectome PHantom (HCPh)**]
  .indent[
  .gray-text[A New Standard for Dense, Multi-Modal Neuroimaging]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Major achievements**]
  .indent[
  .dimgray-text[Standardization in neuroimaging]
  
  .dimgray-text[Open Science advocacy, training, and mentorship]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Grant timeline**]
]

???

Having deeply focused on the standardization to address analytical variability, in 2020 I started to work on the reliability problem of connectivity analysis.

Tat that moment, I identified that the neuroimaging community was lacking an ultra-dense, calibration-grade dataset to address the problem.

With the support of an Ambizione project, I launched the Human Connectome Phantom project.

The idea is to have a single subject—myself—scanned intensively within a very short time window to minimize brain changes (and hence, serve as a "phantom" reference).

The dataset needed to combine sophisticated functional and structural MR acquisitions, and a wealth of physiological tracking.

---

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-00.svg" width="90%" /><br />
]

???

As mentioned, I lent myself as the phantom given the hefty commitment the project involved and the total absence of antecedents of a human being scanned so frequently.


---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-01.svg" width="90%" /><br />
]

???

First, our study focused on a single scanner at CHUV.

We collected a very comprehensive MRI protocol with technical sophistication to provide a solid foundation to secondary methodological developments.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-02.svg" width="90%" /><br />
]

???

We collected eye-tracking to assess arousal and analyze correlation with other facets such as head motion.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-03.svg" width="90%" /><br />
]

???

We also obtained a wealth of signals to investigate physiological variability, including expired oxygen and CO2 (which is far from standard practice but needed).

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration
]

.left-column3[
<img src="../assets/hcph-04.svg" width="90%" /><br />
]

???

This first wave of data was collected in about 3 weeks to ensure minimal changes on the human brain phantom.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-05.svg" width="90%" /><br />
]

???

Data were then standardized, properly stored under version control, and quality-controlled.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-06.svg" width="90%" /><br />
]

???

We preprocessed the data with NiPreps

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-07.svg" width="90%" /><br />
]

???

Next comes connectivity extraction.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-08.svg" width="90%" /><br />
]

???

And this process yielded at least 36 pairs of structural and functional matrices.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-09.svg" width="90%" /><br />
]

???

But we also wanted to test agreement between scanners.

To do so, we modified the protocol so that it would be suitable for the two clinical scanners with more limited performance.

We collected 12 more sessions on the research scanner with the homogenized protocol.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-10.svg" width="90%" /><br />
]

???

And 12 more sessions per scanner on the two clinical scanners.

---
count:false

## HCPh: An ultra-dense, calibration dataset

.right-column3[
Time span: 6 weeks

Functional and Structural Connectivity

Comprehensive physio & eye tracking

Emphasis on technical aspects:

  * fieldmaps
  * high-angular resolution, multishell DWI with doubled-orientations
  * multi-echo fMRI
  * positive-control task
  * breath-holding task

Naturalistic movie (cognition state variability)

Peer-reviewed pre-registration

]

.left-column3[
<img src="../assets/hcph-full.svg" width="90%" /><br />
]

???

Finally, we committed to release all materials (data, code, operating procedures, etc.) open.

---

## HCPh: Registered-report driven

.right-column3[
.larger.center[
.large[**Stage 1 RR**] <br />
Nature Methods

<a href="https://doi.org/10.17605/OSF.IO/VAMQ6"><img src="../ISMRM2024/images/qr-RR.svg" alt="workflow" style="width: 85%" />
Provins et al., (2023)</a>
]
]

.left-column3[
<img src="../assets/hcph-full.svg" width="90%" /><br />
]

???

The study was accepted in October 2023 as a Stage 1 Registered Report at Nature Methods.

The registered report, data collection, and the research was carried out by a large team, with the outstanding role of the Ph.D. student funded by the Ambizione (Celine Provins), and is accessible with the QR code.

By pre-registering, we locked in our hypotheses and methods up front and addressed design issues raised by three reviewers before data collection started.

---

# Why the Registered Report Approach

.boxed-content.large.no-bullet[

* <i class="fa-solid fa-circle-right"></i> .larger[**Pre-Approved Study Design**]
  .indent[
  .gray-text[Hypotheses, protocols, and analyses peer-reviewed before data collection]
  ]

* <i class="fa-solid fa-circle-right"></i> .larger[**Transparency & Rigor**]
  .indent[
  .gray-text[Minimizes biases, “p-hacking,” and the garden of forking paths]
  ]

* <i class="fa-solid fa-circle-right"></i> .larger[**Drawbacks**]
  .indent[
  .gray-text[Challenging pre-registration report preparation, slow review process]
  ]

]

???

Registered Reports reduce biases and ensure transparency as it removes incentives to drive the results and interpretation.

Why? Because with the acceptance of the Stage 1, journals commit to publish the final work (or Stage 2) regardless of the final outcomes.

Given that the HCPh is intended as a foundational reference dataset, it was crucial that the design itself was peer-reviewed.

Registered reports are not free of caveats.

With the communal improvement of scientific endeavors, we will be able to shorten the preparation time of pre-registered analysis and ensure that peer-review does not excessively delays the onset of data collection.

---

# HCPh: Study Outcomes


.boxed-content.no-bullet.pad-top[

* .large.pad-top[<i class="fa-solid fa-database"></i> One-of-a-kind, open **dataset**]
  .indent[
  .gray-text[Thoroughly documented, with open [Standard Operating Procedures (SOPs)](https://www.axonlab.org/hcph-sops)]
  ]

* .large[<i class="fa-solid fa-chart-bar"></i> Within-scanner **reliability characterization**]
  .indent[
  .gray-text[PCA-based modeling of FC residuals]

  .gray-text[Bayesian modeling of SC]
  ]

* .large[<i class="fa-solid fa-scale-balanced"></i> Between-scanner **agreement**]
  .indent[
  .gray-text[One research scanner vs. two clinical scanners]
  ]

]

???

We are currently concluding the final Stage 2 reporting the results, to be submitted to the journal within the next 3-4 months.

We have collected and preprocessed a unique dataset and publicly released its comprehensive standard operating procedures.

Following our pre-registered commitments, we are currently executing our promised analysis involving principal components analyses of functional connectivity and bayesian modelling of structural connectivity.

Finally, we will investigate how structural and functional connectivity measurements agree if obtained with different devices.

This will yield a baseline measure of how stable connectivity metrics can be if the protocol is maximally controlled.

---

# HCPh: Significance
<br />

.boxed-content.no-bullet[

* .large.pad-top[<i class="fa-solid fa-ruler"></i> Complement standardization with **calibration**]
  .indent[
  .gray-text[Provide normative distributions of SC and FC (prior knowledge)]

  .gray-text[An approach to design *calibrated* studies, potentially with the same or new **human phantoms**]
  ]

* .large[<i class="fa-solid fa-route"></i> Methodological **advance**]
  .indent[
  .gray-text[Removal of physiological variability, artifacts, etc.]

  .gray-text[Between-scanner agreement of measurements (other than connectivity)]
  ]

* .large[<i class="fa-solid fa-building-columns"></i> A **foundational dataset**]
  .indent[
  .gray-text[SC & FC training dataset]

  .gray-text[Fully-open, ideal for courses and training]
  ]

]

???

But the impact of HCPh goes far beyond the analyses committed within the Stage 1 report.

We hope to see new “human phantom” datasets at multiple sites, enabling direct cross-site calibration.

HCPh can also serve as ground truth data for AI-driven motion-correction algorithms, generative modeling, or even real-time scanning protocols.

Essentially, we’re complementing standardization efforts (like NiPreps) with a calibration resource that addresses test-retest and multi-site reproducibility.

---
count:false

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  .indent[
  .dimgray-text[Structural and functional connectivity of the brain]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  .indent[
  .dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]
  ]
* <i class="fa-solid fa-circle-right"></i> .larger[**Major achievements**]
  .indent[
  .gray-text[Standardization in neuroimaging]
  
  .gray-text[Open Science advocacy, training, and mentorship]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Grant timeline**]
]

???

Let's move now into highlighting two of my achievements as per the SNSF definition.

---

# Standardization in neuroimaging—NiPreps

.boxed-content[
<br />
.larger.center[
"*analysis-grade*" data <i class="fa-solid fa-circle-right"></i> data **directly consumable by analyses**
]

.pull-left[
<br />
<br />
*Analysis-grade* data is an analogy to the concept of "*sushi-grade (or [sashimi-grade](https://en.wikipedia.org/wiki/Sashimi)) fish*" in that both are:

.large[<i class="fa-solid fa-circle-right"></i> **minimally preprocessed**,]

and

.large[<i class="fa-solid fa-circle-right"></i> **safe to consume** directly.]
]

.pull-right.center[
<img src="../../../talks/assets/nipreps-transparent.png" width="100%" />
<a href="https://www.nipreps.org"><img src="../../../talks/assets/qr-nipreps-url.svg" width="40%" /><br />
www.nipreps.org</a>
]
]

???

In computational neuroscience I'm recognized by my standardization efforts.

NiPreps is a framework for standardized preprocessing pipelines in neuroimaging.

I often use the “sushi-grade” analogy

just like sushi-grade fish is minimally processed but safe to consume,

NiPreps yields “analysis-grade” data, with minimal processing interventions but ready for direct machine learning ingestion.

---

<p align="center">
<img src="../../../talks/assets/nipreps-chart.png" width="63%" /><br />
<em>NiPreps</em> (<a href="https://doi.org/10.31219/osf.io/ujxp6">Esteban et al., 2020</a>)
</p>

???

Here’s the basic architecture.

It’s an ecosystem of modular tools at different levels of abstraction, offering space for AI/ML innovation all accross individual computer vision tasks.

---

# Standardization in neuroimaging—impact

.boxed-content.larger.no-bullet[

* .large[<i class="fa-solid fa-hands-clapping"></i> **Not just wide adoption**]
  .indent[.gray-text[Used in large neuroscientific endeavors: ENIGMA]

  .gray-text[Industry: [Flywheel's gears for *MRIQC* and *fMRIPrep*](https://flywheel.io/insights/blog/comprehensive-support-bids#:~:text=The%20Flywheel%20Gear%20Exchange%20currently,to%20download%20your%20entire), [Amazon Web Services featured *fMRIPrep*](https://aws.amazon.com/blogs/opensource/fmri-data-preprocessing-aws-fmriprep/#:~:text=itself,efforts%20on%20preprocessing%20fMRI%20data), [QMENTA's RS fMRI tool derived from *fMRIPrep*](https://catalog.qmenta.com/tool/resting-state-fmri#:~:text=Catalog%20catalog,Networks%3B%20%E2%80%A2%20Salience%20Network)
  ]
  .gray-text[Relevant works: [Caucheteux, Gramfort & King, (2023)](https://doi.org/10.1038/s41562-022-01516-2), [Tozzi et al., (2024)](https://doi.org/10.1038/s41591-024-03057-9), [Hyon et al., (2020)](https://doi.org/10.1073/pnas.2013606117), [Yamashita et al., (2020)](https://doi.org/10.1371/journal.pbio.3000966).
  ]
  ]

* .large[<i class="fa-solid fa-people-group"></i> **Built a Lasting Community**]
  .indent[
  .gray-text[100+ researchers engaged in the community worldwide]  
  .gray-text[Community is critical to provide a footing for standardization (following BIDS' experience)]
  ]

* .large[<i class="fa-solid fa-industry"></i> **DevOps-inspired standardization**]
  .indent[
  .gray-text[Reducing the gap between research and application reversing the transference flow]
  ]
]

???

Beyond citations and the staggering usage figures I showed in my first presentation, NiPreps is a major achievement by DORA principles.

NiPreps is used in large consortia like ENIGMA, given a foundation to pioneering clinical applications such as Tozzi 2024, and is offered by major industry players like Flywheel, QMENTA, and even AWS workflows.


The NiPreps framework is driven by a community involving more than 100 scientist globally and their adoption continues growing.

That’s the real impact: it’s changing how people conduct imaging studies and ensuring higher data quality across labs.

One distinct reason for success is that we contributed to bridge the gap between research and real-world applicationi by reversing the typical knowledge transference flow:

We integrate software engineering best practices from devops and industry for the standardization and solidification of the neuroimaging workflow.

---

# Open Science Advocacy, Training, & Mentorship

.boxed-content.larger.no-bullet[
* .large[<i class="fa-solid fa-code"></i> **Open Source** and **transparent research**]
  .indent[
  .gray-text[[2,000+ code contributions/years on GitHub](https://github.com/oesteban) across 80+ repositories]

  .gray-text[RRs (two 🡒 Stage 2, one 🡒 Stage 1)]
  ]

* .large[<i class="fa-solid fa-school"></i> **Off-curriculum, hands-on training**]
  .indent[
  .gray-text[NeuroImaging in Practice ([NiPraxis](https://nipraxis.org)) course]  
  .gray-text[[CIBM/fMRIPrep bootcamp](https://cibm.ch/first-ever-cibm-bootcamp-on-fmriprep-a-major-success/)]
  .gray-text[Hackathons, BrainHacks, etc.]
  ]

* .large[<i class="fa-solid fa-handshake-angle"></i> **Mentorship & Peer Support**]
  .indent[
  .gray-text[Active in online support of other scientists]  
  .gray-text[[AxonLab](https://www.axonlab.org) research group (with Provins graduating today!)]
  ]
]

???

My second major achievement revolves around open science advocacy, training, and mentorship.

In practice, this means:

Contributing to over 2,000 code commits per year across more than 80 open repositories.

Embracing Registered Reports to ensure rigorous methodology and better research planification, as opposed to brute-force trial and error.

Indeed, registered reports will become central to prospectively implementing trustworthy and interpretable models, in contrast to retrospective assurance.

Organizing **hands-on training** like **NiPraxis** and the **CIBM/fMRIPrep Bootcamp** so newcomers can quickly adopt reproducible pipelines.

Leading **hackathons** and **BrainHacks** to foster a collaborative environment where diverse skill sets merge.

Finally, **mentorship** is key.

Through my **AxonLab research group** and broader community support, I help early-career scientists gain confidence and skill in open science practices.


(Actually, right after this interview, I’ll be heading to our PhD student Celine Provins’ public defense—an exciting milestone for our open-science journey!)

---
count:false

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Context**]
  .indent[
  .dimgray-text[Structural and functional connectivity of the brain]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**The Human Connectome PHantom (HCPh)**]
  .indent[
  .dimgray-text[A New Standard for Dense, Multi-Modal Neuroimaging]
  ]
* <i class="fa-solid fa-circle-right fa-solid-dim"></i> .larger.dim[**Major achievements**]
  .indent[
  .dimgray-text[Standardization in neuroimaging]
  
  .dimgray-text[Open Science advocacy, training, and mentorship]
  ]
* <i class="fa-solid fa-circle-right fa-solid"></i> .larger[**Grant timeline**]
]

???

Let's move into my future research plans.

---
.boxed-content.centered[
<img src="images/venn-interests.svg" width="99%" />
]

???

This diagram shows the intersection of my core areas: neuroimaging and computational neuroscience, ML/AI and computer vision, and finally Software engineering & standardization (S&S).

I have identified three main funding opportunities which can be loosely assigned to each of these areas.

---
.boxed-content.centered[
<img src="images/venn-grants-1.svg" width="99%" />
]

???

The first is an SNSF Division II proposing to streamline QA/QC within the MRI experiment session before images are reconstructed (and therefore interpretable by the human eye).

The project is not just an idea.

In fact, I responded to an R01 call from the NIH in the US, and despite falling right below the funding line, the project obtained a very good score.

To address the main criticism of the reviewers, I'm collecting preliminary data in collaboration with Dr. Franceschiello (HES-SO and the sense) to secure a well-rounded submission by October 2025.

This project has strong industry connections with MR vendors such as Siemens and could dramatically reduce artifacts early-on and save precious dollars sunk into unusable data as well as participants' time.

I see it as an opportunity to partner with ML experts at Idiap who specialize in signal processing, plus non-academic collaborators in the medical imaging industry.


---
.boxed-content.centered[
<img src="images/venn-grants-2.svg" width="99%" />
]

???

The second is my ERC Consolidator proposal called NETwin (network digital twins for personalized epilepsy).

The idea is to build individualized “digital twins” of a patient’s brain network using SC and FC data, then apply AI-based modeling to predict seizures and treatment outcomes.

This is directly relevant to my background in connectivity, harnessing those improved pipelines from NiPreps and the reliability insights from HCPh.

Like the SNSF project, this proposal derives from previous funding efforts.

---
.boxed-content.centered[
<img src="images/venn-grants-3.svg" width="99%" />
]

???

The third core grant is more speculative and involves industry collaboration through an Innosuisse Innovation project.

To identify potential partners, we plan to hold a dedicated workshop implementing a "continuing education" program offered to Swiss companies.

In addition to potential ex-Idiap figures, I plan to invite Chris Gorgolewski (Antrhopic, and collaborator in the early days of fMRIPrep) and Fabian Pedregosa (Google) as key speakers to ensure the attractiveness.

The concept is to systematically benchmark and calibrate advanced imaging or possibly AI-driven scanning protocols.

This ties nicely to the “human phantom” concept, potentially enabling multi-site calibration with industry-driven R&D to scale it up.

---

# Grant roadmap—core grants

.boxed-content.centered[
<img src="images/grant-timeline-baseline.svg" width="99%" />
]

???

Let's projected these core opportunities into a timeline.

As introduced, I'm already focused on the October 25 deadline for the SNSF project.

Should this first submission be unsuccessful, I would rework the proposal following the feedback and submit again on April 2026.

The next deadline will be for the ERC grant, as early as December 2025 but likely January or Feburary 2026.

This submission is also initiated from a head-start position.

Finally, the Innosuisse Innovation project is left for the second year, to allow the necessary time to identify at least one industrial partner.

---

# Grant roadmap—other

.boxed-content.centered[
<img src="images/grant-timeline-baseline-1.svg" width="99%" />
]

???

Beyond these three core funding opportunities, I've identified other grants to bolster this strategy.

Let's clean up this timeline and leave here only the icons for the three core grants and place these additional opportunities in time.

---
count:false

# Grant roadmap—other

.boxed-content.centered[
<img src="images/grant-timeline.svg" width="99%" />
]

???

I'm planning to be a co-PI on a grant Russ Poldrack is preparing to advance fMRIPrep.

I'm also drafting a Werner-Siemens-Stiftung application, which funds projects with industrial application to move AI into raising the quality levels of neuroimaging in the hospital.

I'm planning to apply with a colleague at University of Copenhagen to a novo nordisk data science grant.

And finally with Prof. Molly Bright through the US NSF and Swiss NSF partnership program.

This timeline is highly dynamic and some other opportunities will be tracked and applied for if suitable.


---

# Conclusion & Next Steps

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-brain"></i> .larger[Leadership in **Neurocomputing & Neuroimaging**]
  .indent[
  .gray-text[Expanding Idiap's AI4Life program, bridging fundamental science and real-world impact]
  ]

* <i class="fa-solid fa-robot"></i> .larger[**ML/AI Integration**]
  .indent[
  .gray-text[Opportunity for cross-disciplinary collaboration inside and beyond Idiap]
  ]

* <i class="fa-solid fa-network-wired"></i> .larger[**Software & Standardization (S&S)**]
  .indent[
  .gray-text[Synergy between academic innovation and real-world deployment of advanced ML solutions]
  ]

* <i class="fa-solid fa-torch"></i> .larger[**Clear roadmap**]
  .indent[
  .gray-text[Two in-progress proposals building on feedback from previous applications, and additional funding opportunities]
  ]
]

???


Thank you for your attention!

I’m excited about initiating and leading a new neuro-computing research area within the AI4Life program at Idiap, blending the institute's focus on AI/ML and software engineering and produce tangible benefits.

I look forward to your questions.

---
layout: false
count: false


.center[
<a href="https://oesteban.github.io/talks/20250306/">
  <img src="images/qr-talk-url.svg" alt="workflow" style="width: 20%" />
  <br />
  https://oesteban.github.io/talks/20250306/
</a>

<br />
<br />

# Thank you

Oscar Esteban &lt;<code>phd@oscaresteban.es</code>>

]

???

</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script>
      // Use: ![:img Alt text with spaces but not commas, 50%](image.png)
      remark.macros.img = function (altText, width) {
        var url = this;
        return '<img alt="' + altText + '" src="' + url + '" style="width: ' + width + '" />';
      };
      // Use: ![:video](10.5129/10234)
      remark.macros.video = function (width) {
        var url = this;
        return '<video src="' + url + '" width="' + width + '" preload="auto" controls />';
      };
      // Use: ![:doi](10.5129/10234)
      remark.macros.doi = function () {
        var doi = this;
        return '<a href="https://doi.org/' + doi + '">' + doi + '</a>';
      };

      var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true,
          countIncrementalSlides: false,
          highlightSpans: true,
          ratio: '16:9'
      });
    </script>
  </body>
</html>
