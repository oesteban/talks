<!DOCTYPE html>
<html>
  <head>
    <title>302.1 | Big data, cloud, infras & HPC</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="../assets/asciinema-player/asciinema-player.css" />
    <link rel="stylesheet" type="text/css" href="../css/slides.css" />
  </head>
  <body>
<script src="../assets/asciinema-player/asciinema-player.min.js"></script>
<script src="https://kit.fontawesome.com/015d4b01de.js" crossorigin="anonymous"></script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>


<textarea id="source">
name: title
layout: true
class: center
---
layout: false
count: false

.center[

<table style="border-collapse: collapse; border: 0; width: 40%; margin: 50px auto">
<tr>
  <td style="border: 0; padding: 0">
    <a href="https://oesteban.github.io/talks/20250410-teaching/">
      <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 200px"></object>
      <br />
      Presentation
    </a>
  </td>
  <td style="border: 0; padding: 0">
    <a href="https://github.com/oesteban/talks/tree/main/20250410-teaching">
      <object type="image/svg+xml" data="images/qr-repo-url.svg" style="width: 200px"></object>
      <br />
      Code
    </a>
  </td>
</tr>
</table>

# 302 Data computation

## .green[302.1 — Big data, cloud, infras & HPC (14 ECTS)]

<br />

Oscar Esteban &lt;<code><a href="mailto:phd@oscaresteban.es">phd@oscaresteban.es</a></code>>

#### 10.04.2025
]

???

Défis et Solutions pour l’Analyse de Grands Volumes de Données

Bonjour à toutes et à tous ! Je suis Oscar Esteban et je vous souhaite la bienvenue dans ce cours « 302.1 — Big data, cloud, infras & HPC ». C’est une introduction aux grands défis du calcul distribué appliqué à des volumes massifs de données, que ce soit en mode cloud ou sur des infrastructures HPC.

Avec le QR code de gauche vous pouvez consulter toute la présentation de cette séance ; pour la parcourir à votre rythme, et même revenir plus tard sur certaines parties.

Au cœur de ma méthodologie d’enseignement, j’encourage l’accès libre aux ressources. C’est pourquoi le QR code de droite mène au dépôt GitHub, où vous retrouverez tout le code source des slides, ainsi que des scripts et exemples supplémentaires. Vous êtes vivement encouragés à contribuer en corrigeant des fautes de frappe, en proposant des améliorations ou en ajoutant des compléments intéressants. Ces contributions seront valorisées : vous serez ajoutés à la liste des contributeurs et vous pourrez gagner quelques points bonus pour la note finale.

---
name: newsection
layout: true

.perma-sidebar[
<p class="rotate">
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0; height: 20px; padding-top: 6px;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
  <span style="padding-left: 10px; font-weight: 600;">302.1 | Big data, cloud, infras & HPC | Course 1</span>
</p>
]

---

# Objectifs généraux du cours


.boxed-content.large[
* faire connaissance avec les **problématiques** qui vont être abordées

  * stockage, calcul et distribution

* **case pratique** : *visualisation graphique des mots significatifs les plus fréquents dans un corpus textuel*

  * plusieurs **millions** de documents
  * brosser un panorama des techniques et des problèmes rencontrés
]

???

Objectifs de ce cours (au sens large) :

1. Découvrir les principales problématiques autour du big data : stockage, calcul et distribution.

2. Aborder un cas pratique orienté « analyse textuelle » : extraire et visualiser les mots les plus fréquents dans un ensemble de documents potentiellement très vaste.

3. Poser un panorama global : depuis la capture des données jusqu’à l’interprétation et la visualisation, en passant par les méthodes de traitement distribué.

---

# Objectifs spécifiques de la séance

.boxed-content.large[
* **Démontrer un pipeline Big Data** minimal pour analyser un corpus avec Spark.

* Illustrer la **logique d’ingestion, de préparation et de comptage** de mots en contexte HPC/Cloud.

* **Limites de la séance** : nous n’aborderons pas en détail la gestion avancée du stockage, la distribution des données à grande échelle ni la comparaison RDD/DataFrame, car le jeu de données reste modeste et la leçon est centrée sur le fonctionnement général du pipeline.
]

???

Nous avons 15 minutes pour :

- Montrer un pipeline Big Data minimal (lecture, transformation, comptage) avec Spark, sur un cluster HPC ou équivalent.

- Expliquer l’ingestion, la préparation des données (stopwords, regex), puis le comptage de fréquences.

- Nous ne parlerons pas en détail de la distribution avancée (cas de milliers de nœuds), ni de la comparaison poussée RDD vs. DataFrame ; nous voulons simplement donner un aperçu fonctionnel et concret.

---

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-00.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

---
count: false

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-01.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

---
count: false

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-02.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

---
count: false

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-03.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

---

# Limites et cadre de la séance

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-04.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Ici, nous insistons sur les limites de l’exemple :

- On n’aborde pas la collecte complexe ni la mise à l’échelle extrême.

- On se concentre sur des fichiers déjà disponibles dans un répertoire « NFS » partagé.

- On montre l’architecture globale, mais on s’arrête à un exercice de comptage de mots pour illustrer la logique d’un pipeline Big Data.


---

# Présentation de Map-Reduce et Spark

.boxed-content[
![Map-Reduce](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2016/11/MapReduce-Way-MapReduce-Tutorial-Edureka-768x339.png)
]

???

Un bref rappel de la philosophie Map-Reduce :

1. On découpe le jeu de données en blocs (Map), chaque bloc est traité par un worker (e.g., extraire des paires (clé, valeur)).

2. On regroupe (Shuffle) puis on agrège (Reduce) pour obtenir le résultat final.

Spark est une évolution plus flexible : 

- Il utilise le principe Map-Reduce tout en optimisant la gestion mémoire (RDD) et en proposant des APIs DataFrame plus simples. 

- Il peut se déployer sur un cluster HPC ou en mode local, etc.

---

# Le jeu de données « 20 Newsgroups »

.boxed-content[

Example: `doc_116.txt`

<br />

```Text
Subject: PC BOARD Layout Help
From: <EH1QC@CUNYVM.BITNET>
Organization: City University of New York
Lines: 8

Hi,  I have a few questions about laying out a PCB. I am using easytrax for dos
which is a great program. But what my question is When laying out traces what
thickness should they be? I am mainly designing low voltage low current boards
for micro controller apps. What should pad sizes be for resistors? I will be
turning to a commercial PCB maker to produce 1's of these boards and I was
wondering what is the minimum distance traces should be from each other. Well
any info would be great. Thanks.
                                                 Anton
```
]

???

Exemple concret : la collection 20 Newsgroups (environ 20 000 messages sur l'Usenet).

- Chaque fichier contient l’en-tête (Subject, From, etc.) et le corps du message.

- Nous allons extraire, nettoyer et compter les mots (hors stopwords, hors chiffres).

- Sur un jeu plus grand (ex. 200 000 ou 2 millions de documents), on verrait réellement le bénéfice d’un traitement distribué. 

---

# Exemple de script *PySpark* (1)

.boxed-content[
```python
#!/usr/bin/env python
"""
Spark job reading the 20 Newsgroups text files from an NFS path,
applying a stopword filter, and printing the top 100 most frequent tokens.
"""

import os
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

DATA_DIR = Path(os.getenv("SCRATCH", str(Path.home()))) / "datasets"
STOPWORDS_PATH = DATA_DIR / "stopwords.txt"
NEWSGROUPS_PATH = DATA_DIR / "newsgroups"

# Create a set of stop words we don't want to count such as "the"
stopword_list = {
  line.strip().lower()
  for line in STOPWORDS_PATH.read_text().splitlines() if line.strip()
}

```
]

???

Ici, on montre le squelette du script : 

- Imports de base (os, Path, SparkSession).

- Définition des chemins : DATA_DIR, STOPWORDS_PATH, NEWSGROUPS_PATH.

- Chargement des stopwords (mots vides) depuis un fichier texte.

Ce script nous servira d’exemple pour présenter le pipeline Spark.

---
count: false

# Exemple de script *PySpark* (2)

.boxed-content[
```python
def main():
    # 1. Initialize Spark
    spark = SparkSession.builder \
        .appName("WordCountOnNFS") \
        .getOrCreate()

    # 2. Read text files as a DataFrame (one column: "value")
    text_df = spark.read.text(f"{NEWSGROUPS_PATH}/*.txt")

    # 3. Split each line into tokens, explode into rows
    words_df = text_df.select(
        F.explode(
            F.split(F.col("value"), r"\\W+")
        ).alias("token")
    )

    # 4. Convert to lowercase
    words_df = words_df.select(F.lower(F.col("token")).alias("token"))

```
]

??? 

Suite du script :

1. Création de la SparkSession (SparkSession.builder…).

2. On lit des fichiers texte depuis NEWSGROUPS_PATH : on obtient un DataFrame avec une colonne "value" par ligne.

3. split/explode : on transforme chaque ligne en plusieurs tokens, et on « explose » pour avoir une ligne par token.

4. Passage en minuscules (lowercase).

On sépare la logique en étapes pour la rendre plus lisible.

---
count: false

# Exemple de script *PySpark* (3)

.boxed-content[
```python
    # 5. Filter out empty strings, numeric tokens, and stopwords
    words_df = words_df.filter(
        (F.col("token") != "") &
        (~F.col("token").rlike("^[0-9]+$")) &
        (~F.col("token").isin(stopword_list))
    )

    # 6. Group by token, count occurrences
    counts_df = words_df.groupBy("token").count()

    # 7. Get the top 15 words by descending frequency
    top_15 = counts_df.orderBy(F.desc("count")).limit(15)

    # 9. Show results
    top_15.show(truncate=False)

    spark.stop()

if __name__ == "__main__":
    main()
```

]

??? 

Dernières étapes :

- On contrôle que les tokens ne soient pas vides, numériques, ou un stopword.

- groupBy + count pour le comptage.

- on ordonne par comptage et on prend les 15 les plus fréquents.

- show pour afficher, puis spark.stop().

Ainsi, on a un pipeline complet, minimal, qui illustre la logique Spark en DataFrame. 


---

# Resilient Distributed Dataset (RDD) vs. DataFrame

.boxed-content[
```python
    # 2. Read text files into RDD 
    text_rdd = spark.sparkContext.textFile(f"{NEWSGROUPS_PATH}/*.txt")

    # 3-4. Convert to lowercase and split into tokens
    tokens_rdd = text_rdd.flatMap(
        lambda line: re.split(r"\W+", line.lower())
    )

    # 5. Filter out empty strings, numeric tokens, and stopwords
    filtered_rdd = tokens_rdd.filter(
        lambda token: (
            token != "" and
            not re.match(r'^[0-9]+$', token) and
            token not in stopword_list
        )
    )

    # Map each token to (token, 1), then reduce
    word_counts = filtered_rdd.map(lambda w: (w, 1)) \
                              .reduceByKey(lambda a, b: a + b)

    # Retrieve the top 15 by descending frequency
    top_15 = word_counts.takeOrdered(15, key=lambda x: -x[1])
```
]

???

La nouvelle infrastructure du Spark optimise et orienté sur les bases de données.

Le système antérieur s'apelle RDD (Resilient Distributed Dataset) et est plus bas-niveau.

- En RDD on manipule map, filter et reduceByKey directement.

---

# Soumission d’un job HPC avec SBATCH (1)

.boxed-content[
```bash
#!/bin/bash
#SBATCH --job-name=SparkWordCount
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --time=01:00:00

module load spark/3.5.5

# 1. Identify allocated nodes
scontrol show hostnames $SLURM_NODELIST > node_list.txt

# 2. Pick the first node as Spark master
MASTER_NODE=$(head -n 1 node_list.txt)

# 3. Start Spark master on MASTER_NODE
if [ "$HOSTNAME" = "$MASTER_NODE" ]; then
  $SPARK_HOME/sbin/start-master.sh
fi
```

]

???

Exemple de script SBATCH :

1. Nom du job, nombre de nœuds, cpus, etc.

2. Charger le module Spark.

3. Identifier les nœuds via scontrol.

4. Démarrer le master si on est sur le nœud principal.

Note : c’est un mode « éphémère » ; on crée un cluster Spark à la volée. 

Faites attention à la variable $HOSTNAME, etc.

---
count: false

# Soumission d’un job HPC avec SBATCH (2)

.boxed-content[
```bash
# 4. Start Spark workers on each allocated node
for node in $(cat node_list.txt); do
  srun -N1 -w "$node" "$SPARK_HOME/sbin/start-worker.sh" "spark://$MASTER_NODE:7077"
done

# 5. Submit the Spark job
$SPARK_HOME/bin/spark-submit \
  --master spark://$MASTER_NODE:7077 \
  --deploy-mode client \
  $PWD/wordcount.py

# 6. Stop Spark workers
for node in $(cat node_list.txt); do
  ssh $node "$SPARK_HOME/sbin/stop-worker.sh"
done

# 7. Stop Spark master on MASTER_NODE
if [ "$HOSTNAME" = "$MASTER_NODE" ]; then
  $SPARK_HOME/sbin/stop-master.sh
fi
```

]

???

- On lance un worker Spark par nœud (srun).

- On exécute spark-submit pour lancer le job Python (wordcount.py) en mode client.

- Ensuite, on arrête les workers (ssh $node "stop-worker.sh"), puis on arrête le master.

L’ordre est important pour éviter que Spark ne s’arrête avant la fin du job. 

Dans un HPC typique, on utilise parfois ssh plutôt que srun pour que les daemons restent en arrière-plan.

---

# Démo pratique

.boxed-content[
<div class="asciicast" id="636539" style="padding-top: 25px"></div>
]

---

# Analyse des résultats (`top15_output.txt`)

.boxed-content[
.large[
```Text
one	14635
writes	13344
article	12275
people	9473
university	9431
know	8631
get	8343
think	7602
time	6711
use	6673
new	6385
good	5946
even	5380
see	5191
god	5062
```
]
]

??? 
Ici, on voit un exemple de top 15 mots les plus fréquents :

  one, writes, article, people, etc.

On peut interpréter ces résultats. S’il reste des tokens parasites (ex. >, lines, etc.),
il faudra affiner le regex ou la suppression de l’en-tête. 

Cela donne une idée du pipeline final.

---

# Synthèse finale

???

Conclusion :

- Nous avons vu un mini pipeline Big Data (Spark + HPC), 
- Bien que l’exemple soit modeste (20 000 documents), la méthode est la même pour des ensembles beaucoup plus grands.
- L’objectif était de démontrer la logique Spark : configuration du cluster, script PySpark, et soumission HPC.
- N’hésitez pas à tester des datasets plus volumineux ou plus variés (Enron emails, Amazon reviews, Wikipedia…).

---

#  Devoirs et pistes d’approfondissement

???

Pistes pour aller plus loin :

1. Approfondir la partie ingestion (ex. streaming, logs, ingestion depuis le cloud).

2. Expérimenter RDD vs. DataFrame sur un corpus plus lourd, comparer performances.

3. Mettre en place un pipeline complet avec un stockage distribué (S3, HDFS) et 
   observer l’impact sur le temps de traitement.
  
4. Ajouter un module de visualisation (par exemple, générer un nuage de mots, 
   ou construire un mini-dashboard).

5. Proposer des améliorations (pull requests) dans le dépôt GitHub pour ajouter 
   d’autres exemples de scripts ou de doc.
   
Vos contributions sont les bienvenues !


</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script>
      // Use: ![:img Alt text with spaces but not commas, 50%](image.png)
      remark.macros.img = function (altText, width) {
        var url = this;
        return '<img alt="' + altText + '" src="' + url + '" style="width: ' + width + '" />';
      };
      // Use: ![:video](10.5129/10234)
      remark.macros.video = function (width) {
        var url = this;
        return '<video src="' + url + '" width="' + width + '" preload="auto" controls />';
      };
      // Use: ![:doi](10.5129/10234)
      remark.macros.doi = function () {
        var doi = this;
        return '<a href="https://doi.org/' + doi + '">' + doi + '</a>';
      };

      var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true,
          countIncrementalSlides: false,
          highlightSpans: true,
          ratio: '16:9'
      });

      // Now retrieve all IDs of asciinema casts
      const allcasts = new Map();

      slideshow.on('afterShowSlide', function (slide) {
        // Slide is the slide being navigated
        var slideNumber = slide.getSlideIndex();
        var element = document.getElementsByClassName("remark-visible")[0].getElementsByClassName('asciicast')
        if (element.length == 0 ) {
          return;
        }

        if (allcasts.has(slideNumber)) {
          allcasts.get(slideNumber).play();
          return;
        }

        var castid = element[0].attributes["id"].value;
        allcasts.set(slideNumber, AsciinemaPlayer.create(
            'images/spark.cast',
            document.getElementById('636539'),
            { autoPlay: true, speed: 3, idle_time_limit: 8, rows: 20, cols: 100 }
        ));

      });
      slideshow.on('beforeHideSlide', function (slide) {
        // Slide is the slide being navigated
        var slideNumber = slide.getSlideIndex();
        if (allcasts.has(slideNumber)) {
          allcasts.get(slideNumber).pause();
        }
      });
    </script>
  </body>
</html>
