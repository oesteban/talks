<!DOCTYPE html>
<html>
  <head>
    <title>302.1 | Big data, cloud, infras & HPC</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="../assets/asciinema-player/asciinema-player.css" />
    <link rel="stylesheet" type="text/css" href="../css/slides.css" />
  </head>
  <body>
<script src="../assets/asciinema-player/asciinema-player.min.js"></script>
<script src="https://kit.fontawesome.com/015d4b01de.js" crossorigin="anonymous"></script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>


<textarea id="source">
name: title
layout: true
class: center
---
layout: false
count: false

.center[

<table style="border-collapse: collapse; border: 0; width: 40%; margin: 50px auto">
<tr>
  <td style="border: 0; padding: 0">
    <a href="https://oesteban.github.io/talks/20250410-teaching/">
      <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 200px"></object>
      <br />
      Presentation
    </a>
  </td>
  <td style="border: 0; padding: 0">
    <a href="https://github.com/oesteban/talks/tree/main/20250410-teaching">
      <object type="image/svg+xml" data="images/qr-repo-url.svg" style="width: 200px"></object>
      <br />
      Code
    </a>
  </td>
</tr>
</table>

# 302 Data computation

## .green[302.1 — Big data, cloud, infras & HPC (14 ECTS)]

<br />

Oscar Esteban &lt;<code><a href="mailto:phd@oscaresteban.es">phd@oscaresteban.es</a></code>>

#### 10.04.2025
]

???

Bonjour à tous ! Je suis Oscar Esteban et je vous souhaite la bienvenue dans ce cours « 302.1 — Big data, cloud, infras & HPC ».

C’est une introduction aux grands défis du calcul distribué appliqué à des volumes massifs de données (ou Big Data), que ce soit en mode cloud ou sur des infrastructures HPC.

Avec le QR code de gauche vous pouvez consulter toute la présentation de cette séance ; pour la parcourir à votre rythme, et même revenir plus tard sur certaines parties.

Au cœur de ma méthodologie d’enseignement, j’encourage l’accès libre aux ressources.

C’est pourquoi le QR code de droite mène au dépôt GitHub, où vous retrouverez tout le code source des slides, ainsi que des scripts et exemples supplémentaires.

Vous êtes vivement encouragés à contribuer en corrigeant des fautes de frappe, en proposant des améliorations ou en ajoutant des compléments intéressants.

Ces contributions seront valorisées : vous serez ajoutés à la liste des contributeurs et vous pourrez gagner quelques points bonus pour la note finale.

---
name: newsection
layout: true

.perma-sidebar[
<p class="rotate">
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0; height: 20px; padding-top: 6px;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
  <span style="padding-left: 10px; font-weight: 600;">302.1 | Big data, cloud, infras & HPC | Course 1</span>
</p>
]

---

# Objectifs généraux du cours


.boxed-content.large.no-bullet[
* <i class="fa-solid fa-globe"></i> **Big Data**: défis et solutions
  
  .gray-text[
  * stockage, calcul et distribution
  ]
  

* <i class="fa-solid fa-person-dots-from-line"></i> Avec un **cas pratique** :

  > *visualisation graphique des mots significatifs les plus fréquents dans un corpus textuel*

  * plusieurs **millions** de documents
  * proposer un panorama des techniques et des problèmes rencontrés
]

???

Ce cours aborde les principaux défis et solutions pour analyser des données massives (Big Data), avec un accent sur le stockage, le calcul et la distribution.

Nous illustrerons ces aspects à travers un cas pratique : la visualisation des mots les plus significatifs dans un corpus textuel pouvant atteindre plusieurs millions de documents.

L’objectif est de dresser un panorama des techniques existantes pour résoudre les problèmes concrets que l’on rencontre lorsqu’on manipule de très grands volumes de données.

<!--
This course is about the challenges and solutions for analyzing Big Data, focusing on storage, computation, and distribution.

We will approach this goal through a practical case study: visualizing the most significant words in a potentially vast text corpus.

By vast we mean several million documents, and this example will allow us to map what's available to us in terms of techniques to resolve the problems we encounter.
-->

---

# Objectifs spécifiques de la séance

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-wrench"></i> **Démontrer un pipeline avec Spark** pour compter les mots les plus fréquents dans un corpus de text.

* <i class="fa-solid fa-server"></i> Illustrer la **execution sur HPC**.

<br />

* <i class="fa-solid fa-circle-stop"></i> **Limites de la séance** :

  .gray-text.indent[
* gestion avancée du stockage,
* la distribution des données et de compute,
* execution in Cloud, ou
* utilisation finale: visualisation, l’analyse, etc.
  ]
]

???

Aujourd’hui, nous allons amorcer cette thématique en démontrant un pipeline minimal, basé sur Spark, pour compter les mots les plus fréquents dans un corpus d’environ 20 000 documents.

Nous distribuerons le calcul sur plusieurs nœuds de notre cluster.

Cela nous permettra d’illustrer la mise en place d’un pipeline Big Data et son exécution sur un système HPC.

Pour simplifier, nous ne traiterons pas :
- de l’infrastructure de stockage avancée,
- de la distribution fine des données,
- du déploiement dans le cloud,
- ni de l’utilisation finale des résultats (visualisation, analyse approfondie).

C'est parti !

<!--
Today, we will initiate this exciting journey by demonstrating a minimal pipeline using Spark to count the most frequent words in a text corpus of about 20,000 documents.

We will distribute the computation across several nodes in our cluster.

This will allow us to illustrate the creation of a Big Data pipeline and its execution on a HPC system.

In order to keep the example simple, we will not cover advanced topics such as:

- advanced network and distributed storage alternatives,

- the details and specifics of workload distribution,

- the deployment of the pipeline in a cloud environment,

- or the final use of the results, such as visualization or analysis.

Let's get started!

-->

---

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-00.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Un pipeline Big Data - c'est quoi?

C’est un ensemble de processus permettant de collecter, stocker, traiter et analyser de grands volumes de données.

Typiquement, on commence par :

- **Ingestion des données** : s'agit de la récupération depuis diverses sources (comme des bases, des APIs, des flux, etc.).

- **Stockage** : utilisation d’un système distribué ou d’une base adaptée pour manipuler d’importants volumes

Nous allons voir comment ces étapes s’organisent pour former un flux cohérent.

<!--

So, what is a Big Data pipeline?

A Big Data pipeline is a series of processes that allow us to collect, store, process, and analyze large volumes of data.

It typically consists of several stages, including:

- **Data Ingestion**: Collecting data from various sources, such as databases, APIs, or streaming platforms.

- **Data Storage**: Storing the data in a distributed file system or a database that can handle large volumes of data.
  The industry talks about "data lakes" or "data warehouses", which are build based on technologies like HDFS, S3, and databases.
-->

---
count: false

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-01.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Une fois les données rassemblées, on passe à la préparation (ou "preprocessing") :

- Nettoyage ou filtrage pour éliminer le bruit, les doublons, ou des informations non pertinentes.

- Fusion ou agrégation depuis plusieurs sources, pour obtenir un ensemble de données unifié.

- Transformation (normalisation, réduction de dimensions, etc.).

- Extraction de traits pertinents pour l’analyse.

C’est une étape cruciale, car la qualité du prétraitement influence directement la fiabilité des résultats finaux.

<!--
Once data munging is resolved, we move on to the preparation of data.

Data preprocesing involves:

- cleaning or filtering to remove noise, duplicates, or irrelevant information,

- merging or aggregating data from different sources to create a unified dataset,

- transforming data, for example, normalizing or reducing dimensionality, and

- extracting features or relevant information for analysis.
-->

---
count: false

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-02.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Après le preprocessing, on aborde la phase d’analyse.

On applique par exemple :

- Des méthodes statistiques, d’apprentissage automatique ou de deep learning,

- qui peuvent etre mises en place avec des frameworks comme TensorFlow, PyTorch, Scikit-learn, etc.

Cette étape se déroule souvent sur une infrastructure adaptée (cloud, cluster HPC),

où nous devons gérer le partage des ressources, la parallélisation et la mémoire.

<!--
After preprocessing, data are suitable for analysis.

This stage involves applying various techniques to extract insights or knowledge from the data, including:

- statistical analysis, machine learning, or deep learning algorithms, which

- are available under different frameworks (e.g., TensorFlow, PyTorch, Scikit-learn, etc.).

- These models are then executed, which will involve choices in infrastructure (e.g., cloud, HPC, etc.), and

- within that choice allocating the adequate resources for workload distribution, memory management, and so on.
-->

---
count: false

# Présentation globale du workflow Big Data

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-03.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Enfin, on utilise effectivement les résultats :

- Sous forme de rapports et de visualisations,

- Pour effectuer des prédictions,

- Ou encore pour aider à la prise de décision (business intelligence).

Ce dernier stade permet de valoriser tout le pipeline : au-delà de l’analyse, on doit rendre les conclusions exploitables et compréhensibles.

<!--
Finally, we actually **use** the results of our analysis:

- perhaps in reporting and visualization,

- making predictions about the future, or

- supporting decision making and business intelligence services.
-->

---

# Limites et cadre de la séance

.boxed-content[
<object type="image/svg+xml" data="images/big-data-workflow-04.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Aujourd’hui, nous allons surtout illustrer la partie “modélisation/calcul” :

- Nous mettrons en place un mini-pipeline Spark pour compter les mots les plus fréquents dans un corpus textuel de taille modeste,

- Et nous le ferons tourner sur un cluster HPC.

Le but est de montrer un exemple concret de pipeline Big Data, même si nous ne couvrirons pas toutes les étapes ni tous les détails.

<!-- 
Most of today's session focuses on the modeling step:

- we will develop a simple "model" with Spark to count the most frequent words in small-ish text corpus,

- and demonstrate how to run it on a HPC cluster.
-->

---

# Présentation de Map-Reduce

.boxed-content[
<object type="image/svg+xml" data="images/map-reduce-00.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Le cadre historique le plus célèbre pour le Big Data est Map-Reduce, introduit par Google en 2004.

Map-Reduce est à la fois un modèle de programmation et une mise en œuvre pour traiter de grands jeux de données en parallèle sur un cluster.

Comment ça fonctionne ?

- On part d’un ensemble massif de documents (parfois millions de fichiers).

<!-- 
Perhaps, the most well-known framework for Big Data processing is Map-Reduce, which was introduced by Google in 2004.

MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel and distributed algorithm on a cluster.

How does it work?

- At the input we have Big Data, for example millions of text documents.

-->

---
count: false

# Présentation de Map-Reduce

.boxed-content[
<object type="image/svg+xml" data="images/map-reduce-01.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Ensuite, on divise ces documents en “chunks” plus petits (ex. lignes ou paragraphes).

Chaque chunk est donc un lot de texte partiel, prêt à être analysé séparément dans le cluster.

<!-- 
Then those documents are split into smaller chunks (for example, lines or paragraphs).

Each chunk may then be split into words, which we call "tokens" here.
-->

---
count: false

# Présentation de Map-Reduce

.boxed-content[
<object type="image/svg+xml" data="images/map-reduce-02.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Dans la phase “map”, on convertit ces chunks en tokens (les mots), puis on leur associe une valeur (par exemple 1 occurrence).

Ce travail est distribué : chaque nœud du cluster traite une partie de l’ensemble.

<!-- 
The original Map-Reduce approch distributes then the tokens into the available nodes of the cluster, which execute a task.

The task, in this case, is just simply assigning a frequency of 1 to each token.

This parallel task is called the "Map" step.
-->

---
count: false

# Présentation de Map-Reduce

.boxed-content[
<object type="image/svg+xml" data="images/map-reduce-03.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Après la map, on effectue le “shuffle” : c’est la redistribution des tokens par clé (donc par mot), sur les différents nœuds.

Cela permet de regrouper tous les mêmes mots au même endroit, en vue de l’étape suivante.

<!-- 
Map is followed by the Shuffle step, in which the tokens are redistributed across the nodes grouped by their key (the token itself).
-->

---
count: false

# Présentation de Map-Reduce

.boxed-content[
<object type="image/svg+xml" data="images/map-reduce-04.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Arrive ensuite l’étape “reduce” : pour chaque mot, on cumule les occurrences provenant de l’ensemble des mappeurs.

On obtient alors le total d’apparitions de chaque mot dans la collection.

<!-- 
At this point, data are aggregated and "reduced" by summing the total frequency of each token.

The result of reduce in this case is simply a list of tokens and their frequencies.
-->

---
count: false

# Présentation de Map-Reduce

.boxed-content[
<object type="image/svg+xml" data="images/map-reduce-05.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Enfin, on formate et on écrit les résultats sur le disque (ou dans une base).

Dans le cas présent, ce serait la liste des mots, avec leur fréquence associée.

<!-- 
Then, the result is formatted and written to disk.
-->

---
count: false

# Présentation de Map-Reduce

.boxed-content[
<object type="image/svg+xml" data="images/map-reduce-06.svg" style="width: 100%; padding-top: 20pt;"></object>
]

???

Spark, Dask, Flink, etc., sont nés en grande partie pour aller plus loin que ce schéma, 
mais ils conservent tous l’idée fondamentale : répartir le travail de map, puis regrouper, 
puis réduire.

Maintenant, voyons concrètement comment appliquer ces concepts.

<!-- 
Spark and other frameworks (e.g., Dask, Flink, etc.) were originally built around this paradigm and now offer more advanced features.

Let's now apply these fundamental concepts.
-->

---

# Le jeu de données « 20 Newsgroups »

.boxed-content[

Example: `doc_116.txt`

<br />

```Text
Subject: PC BOARD Layout Help
From: <EH1QC@CUNYVM.BITNET>
Organization: City University of New York
Lines: 8

Hi,  I have a few questions about laying out a PCB. I am using easytrax for dos
which is a great program. But what my question is When laying out traces what
thickness should they be? I am mainly designing low voltage low current boards
for micro controller apps. What should pad sizes be for resistors? I will be
turning to a commercial PCB maker to produce 1's of these boards and I was
wondering what is the minimum distance traces should be from each other. Well
any info would be great. Thanks.
                                                 Anton
```
]

???

Nous allons utiliser le dataset 20 Newsgroups de Scikit-learn, soit ~20 000 messages Usenet.

Chaque fichier comporte un en-tête (Subject, From, etc.) qui contient souvent des mots répétés (ex. "university"). Il faudra donc filtrer ou ignorer ces tokens si on veut des statistiques plus pertinentes.

Nous supposerons que tous ces fichiers sont stockés dans un répertoire partagé NFS sur le cluster.

<!-- 
We will employ Scikit-learn's 20 Newsgroups dataset, which is a collection of approximately 20,000 messages in newsgroups lists like the one on screen.

All messages have a header, which contains some tokens that will seen inflated frequencies such as subject, from, organization and lines.

We will need to remove these tokens from our analysis.

We will assume all these messages are under a shared NFS path in our cluster.
-->

---

# Exemple de script *PySpark* (1)

.boxed-content[
```python
#!/usr/bin/env python
"""
Spark job reading the 20 Newsgroups text files from an NFS path,
applying a stopword filter, and printing the top 100 most frequent tokens.
"""

import os
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

DATA_DIR = Path(os.getenv("SCRATCH", str(Path.home()))) / "datasets"
STOPWORDS_PATH = DATA_DIR / "stopwords.txt"
NEWSGROUPS_PATH = DATA_DIR / "newsgroups"

# Create a set of stop words we don't want to count such as "the"
stopword_list = {
  line.strip().lower()
  for line in STOPWORDS_PATH.read_text().splitlines() if line.strip()
}

```
]

???

Passons au code.

Nous commençons par créer un script Python baptisé `wordcount.py`.

- On utilise PySpark, l’API Python de Spark.

- On définit les chemins pour le données.

- On lit le fichier de stopwords pour éliminer ces mots lors du comptage.

C’est le squelette de base avant de manipuler le pipeline Spark proprement dit.

<!-- 
Now, let's get our hands dirty with some code.

We will use PySpark, which is the Python API for Apache Spark.

PySpark allows us to write Spark applications in Python, and it is widely used for data processing and analysis.

The first part of our implementation is straightforward:

- We import the necessary libraries and modules.

- We define the paths to our datasets, including the stopwords and the newsgroups data.

- We read the stopwords file and create a set of stopwords that we will use to filter out unwanted tokens.
-->

---
count: false

# Exemple de script *PySpark* (2)

.boxed-content[
```python
def main():
    # 1. Initialize Spark
    spark = SparkSession.builder \
        .appName("WordCountOnNFS") \
        .getOrCreate()

    # 2. Read text files as a DataFrame (one column: "value")
    text_df = spark.read.text(f"{NEWSGROUPS_PATH}/*.txt")

    # 3. Split each line into tokens, explode into rows
    words_df = text_df.select(
        F.explode(
            F.split(F.col("value"), r"\\W+")
        ).alias("token")
    )

    # 4. Convert to lowercase
    words_df = words_df.select(F.lower(F.col("token")).alias("token"))

```
]

???

Ici, nous :

1. Créons la SparkSession,

2. Lisons les fichiers texte sous forme de DataFrame (une ligne = une row),

3. Divisons chaque ligne en tokens via split + explode,

4. Convertissons en minuscules.

Cela prépare notre corpus à être filtré et compté.

<!-- 
Then, we initialize Spark and preprocess the dataset:

- We start the Spark session.

- We read the text files into a DataFrame, where each line is a row in the DataFrame.

- We split each line into tokens using a regular expression and explode the DataFrame to create a new row for each token.

- We convert all tokens to lowercase to ensure uniformity.
-->

---
count: false

# Exemple de script *PySpark* (3)

.boxed-content[
```python
    # 5. Filter out empty strings, numeric tokens, and stopwords
    words_df = words_df.filter(
        (F.col("token") != "") &
        (~F.col("token").rlike("^[0-9]+$")) &
        (~F.col("token").isin(stopword_list))
    )

    # 6. Group by token, count occurrences
    counts_df = words_df.groupBy("token").count()

    # 7. Get the top 15 words by descending frequency
    top_15 = counts_df.orderBy(F.desc("count")).limit(15)

    # 9. Show results
    top_15.show(truncate=False)

    spark.stop()

if __name__ == "__main__":
    main()
```

]

???

Ensuite :

- On filtre les tokens vides, purement numériques ou dans la liste de stopwords.

- On groupe par "token" pour compter les occurrences (c’est l’opération de reduce).

- On extrait les 15 tokens les plus fréquents et on affiche.

Spark exécute ces étapes en parallèle sur le cluster, de façon transparente pour nous.

<!-- 
A final preprocessing is necessary (number 5), which eliminates empty tokens, numeric tokens, and stopwords.

Then we group the DataFrame by token and count the occurrences of each token, which would implement the map-shuffle-reduce operation in a highly optimized and distributed way with Spark.

Finally, we retrieve the top 15 tokens by descending frequency and display the results.
-->

---

# Resilient Distributed Dataset (RDD) vs. DataFrame

.boxed-content[
```python
    # 2. Read text files into RDD 
    text_rdd = spark.sparkContext.textFile(f"{NEWSGROUPS_PATH}/*.txt")

    # 3-4. Convert to lowercase and split into tokens
    tokens_rdd = text_rdd.flatMap(
        lambda line: re.split(r"\W+", line.lower())
    )

    # 5. Filter out empty strings, numeric tokens, and stopwords
    filtered_rdd = tokens_rdd.filter(
        lambda token: (
            token != "" and
            not re.match(r'^[0-9]+$', token) and
            token not in stopword_list
        )
    )

    # 6. Map each token to (token, 1), then reduce
    word_counts = filtered_rdd.map(lambda w: (w, 1)) \
                              .reduceByKey(lambda a, b: a + b)

    # 7. Retrieve the top 15 by descending frequency
    top_15 = word_counts.takeOrdered(15, key=lambda x: -x[1])
```
]

???

Le script précédent utilise l’API DataFrame, plus moderne et optimisée.

On peut cependant réaliser le même traitement avec l’API RDD “historique” :

- On lit textFile(...),

- On flatMap pour extraire les tokens,

- On filter pour éliminer ce qu’on ne veut pas,

- On map -> (token,1) + reduceByKey pour agréger.

Bien que l’API RDD soit plus bas niveau, la logique est identique au paradigme map-reduce traditionnel.

<!-- 
Our script uses the newest, recommended DataFrame API, which is optimized for performance and ease of use.

However, we can also implement the same logic using the older RDD API.

In the interest of time, just focus on step 6 where map and reduce (in this case, reduceByKey) are explicitly defined.
-->

---

# Soumission d’un job HPC avec SBATCH (1)

.boxed-content[
```bash
#!/bin/bash
#SBATCH --job-name=SparkWordCount
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --time=01:00:00

module load spark/3.5.5

# 1. Identify allocated nodes
scontrol show hostnames $SLURM_NODELIST > node_list.txt

# 2. Pick the first node as Spark master
MASTER_NODE=$(head -n 1 node_list.txt)

# 3. Start Spark master on MASTER_NODE
if [ "$HOSTNAME" = "$MASTER_NODE" ]; then
  $SPARK_HOME/sbin/start-master.sh
fi
```

]

???

Nous avons un script SBATCH pour lancer l’application sur un cluster HPC :

1. Paramètres du job (nom, nœuds, CPUS, etc.).

2. module load spark pour charger Spark.

3. On récupère la liste des nœuds (node_list.txt).

4. On définit le premier nœud comme master, et on y lance le master Spark.

C’est l’approche dite “éphémère” : on instancie Spark à la volée dans le job.

<!-- 
Now, we have our pipeline defined and uploaded to an appropriate path in the cluster with the name `wordcount.py`.

We can now submit our job to the HPC cluster using SLURM.

Let's quickly get an overview of the SBATCH script, which is one standard option to submit jobs to our cluster.

The file has a header with job metadata and resource requirements.

Then, we set up the environment by loading the Spark module.

Because Spark is not permanently deployed on the cluster, we need to create a "Spark network" on the fly.

To do this, we designate one of the nodes the cluster provides us as the master.

Spark will use this node to orchestrate the data distribution and computation.
-->

---
count: false

# Soumission d’un job HPC avec SBATCH (2)

.boxed-content[
```bash
# 4. Start Spark workers on each allocated node
for node in $(cat node_list.txt); do
  srun -N1 -w "$node" "$SPARK_HOME/sbin/start-worker.sh" "spark://$MASTER_NODE:7077"
done

# 5. Submit the Spark job
$SPARK_HOME/bin/spark-submit \
  --master spark://$MASTER_NODE:7077 \
  --deploy-mode client \
  $PWD/wordcount.py

# 6. Stop Spark workers
for node in $(cat node_list.txt); do
  ssh $node "$SPARK_HOME/sbin/stop-worker.sh"
done

# 7. Stop Spark master on MASTER_NODE
if [ "$HOSTNAME" = "$MASTER_NODE" ]; then
  $SPARK_HOME/sbin/stop-master.sh
fi
```

]

???

- On lance un worker sur chaque nœud pour exécuter les tâches.

- On exécute spark-submit pour soumettre le job (wordcount.py) au master.

- À la fin, on arrête les workers et le master.

Cela illustre la configuration d’un mini cluster Spark au sein d’un job SLURM, puis la soumission d’un pipeline PySpark.

<!-- 
We create "workers" on the other nodes, which will be used to execute the tasks.

The workers will connect to the master node.

Then, we submit the Spark job using the `spark-submit` command (see step 5), specifying the master node and the path to our script.

Finally, we stop the workers and the master node to clean up.
-->

---

# Démo pratique ("cooking show")

<br />
.boxed-content.no-bullet.larger[
- .large[<i class="fa-solid fa-kitchen-set"></i> Tous les ingrédients et outils sont prêts :]

  - architecture générale

  - dataset + stopwords

  - script `wordcount.py`

  - script SBATCH

<br />

- .large[<i class="fa-solid fa-fire-burner"></i> Il ne reste plus qu’à « cuire » la recette]
]

???

À ce stade, nous avons fait la mise en place avec tous les ingrédients nécessaires pour lancer notre job :

- Nous avons vu l’architecture globale,

- Nous disposons du dataset et d’un fichier de stopwords,

- Nous avons le script `wordcount.py`,

- Et nous avons aussi le script SBATCH.

Maintenant, comme dans une émission de cuisine (« cooking show »), nous allons simplement exécuter le job pour voir les résultats.

Si vous suivez depuis votre ordinateur, vous pouvez faire des pauses, copier/coller, car il ne s’agit pas d’une vidéo réelle.
Vous pourrez aussi rejouer la démonstration plus lentement, car ici elle sera diffusée à vitesse légèrement accélérée.

<!-- 
At this point, we have all the ingredients ready to run our job.

- We have seen the overall architecture of the solution

- We have a dataset and a list of "stopwords" to filter out.

- We have the script `wordcount.py`,

- and we have the SBATCH script.

Now, like in a cooking show, we will run the job and see the results.

If you are following on your laptop, you can stop it and copy and paste because
it's not a real video.

You can also revisit it and play it slower, as this one will play at 2x speed.
-->

---

.boxed-content[
<div class="asciicast" id="636539" style="padding-top: 25px"></div>
]

???

Comment exécuter concrètement ?

1. Se connecter par ssh au cluster.

2. Vérifier qu’on dispose de stopwords.txt, du dossier newsgroups, etc.

3. Visualiser wordcount.py et le script SBATCH.

4. Lancer sbatch spark.sbatch.

5. Suivre l’avancement via squeue ou sacct.

6. Examiner les logs pour voir si tout s’est bien passé et récupérer les résultats.

7. Vérifier la durée du job avec sacct -j <ID> ou seff <ID>.

C’est un exemple accéléré, façon “cooking show”.

<!-- 
How do we run the job?

We first log-in into the cluster with ssh.

Once logged in, we can see the stopwords file, which was generated by expanding the stopwords in the ntlk package.

We then check one example document, under the `newsgroups` folder.

We can see it is similar to the example we saw before.

Now, we change directories into a folder under home, where we have the script `wordcount.py` and the SBATCH script `spark.sbatch`.

We visualize the `wordcount.py` script, check everything looks good.

We visualize the SBATCH script, and we can see it is similar to the one we just saw.

And we are ready to submit it to the cluster with `sbatch spark.sbatch`.

We can now wait and see when the job is assigned resources with `watch squeue -u $USER`.

When the job enters the queue, we can follow logs.

First of the sbatch script, stored as prescribed by the sbatch header.

And then of Spark, which is written the standard error stream.

After barely a minute, the job finishes, and we can see the output of the script.

We can also check some runtime information with `sacct -j` followed by the job ID.

We finally see some performance metrics with `seff -j` followed by the job ID.
-->

---

# Analyse des résultats (`top15_output.txt`)

.boxed-content[
.large[
```Text
one	14635
writes	13344
article	12275
people	9473
university	9431
know	8631
get	8343
think	7602
time	6711
use	6673
new	6385
good	5946
even	5380
see	5191
god	5062
```
]
]

???

Voici le fichier final listant les 15 tokens les plus fréquents.

On observe “one”, “writes”, “article”... 

<!--
Ce dernier, par exemple, reflète le fait que beaucoup de messages citent “article”.
De même, “university” apparaît souvent en raison des en-têtes ou des signatures.
-->

Dans un usage réel, on peaufinerait encore le filtrage (en-têtes, etc.) pour plus de pertinence.

<!-- 
And this is the file written out by the script, which contains the top 15 tokens and their frequencies.

As we can see, the most frequent token is "one", followed by "writes", "article", and so on.

"writes" is unsurprising, as it is added automatically in newsgroups when responding to previous messages.

"university" also appears very often in the headers because most of the newsgroups are related to universities.
-->

---

# Synthèse finale

.boxed-content.large.no-bullet[
* <i class="fa-solid fa-globe"></i> **Aperçu global** d’un pipeline Big Data<br />
  .gray-text.indent[et des principaux défis (ingestion, distribution, solutions)]

* <i class="fa-solid fa-diamond"></i> **Introduction** du paradigme Map-Reduce<br />
  .gray-text[point de départ historique pour l’analyse à grande échelle]

* <i class="fa-solid fa-trowel-bricks"></i> **Implémentation** d’un mini-pipeline Spark<br />
  .gray-text[appliqué à un jeu de données ouvert]

* <i class="fa-solid fa-server"></i> **Exécution** sur un cluster HPC (via SLURM)<br />
  .gray-text[illustrant les enjeux et bénéfices du calcul distribué]
]

???

Aujourd’hui, nous avons :

- brossé un aperçu du pipeline Big Data et des principaux défis (ingestion, distribution, etc.),

- introduit le paradigme Map-Reduce, base historique de l’analyse de grande envergure,

- implémenté un mini-pipeline Spark sur un dataset ouvert,

- et exécuté le tout sur un cluster HPC (via SLURM), en montrant comment les tâches se distribuent.

C’est un premier jalon. Dans la suite, on pourra approfondir d’autres aspects (streaming, ML, Cloud, etc.).

<!-- 
Today, we've had a brief overview of the Big Data pipeline, anticipating the most relevant challenges and solutions.

We have introduced the Map-Reduce paradigm, which kickstarted the race for high-throughtput Big Data analysis.

Once those concepts were described, we had our hands on the code and implemented a mini pipeline with Spark to process an open datatet.

We have executed the pipeline on a HPC cluster using SLURM, which has allowed us to introduce some of the challenges of distributed computing.
-->

---

# Devoirs et pistes d’approfondissement

## Devoir
.large[
*Remove sorting and clipping the top 15 tokens, generating a visual word cloud instead with Python.*
]

## Pour aller plus loin
.boxed-content.large[
* Convertir le script `wordcount.py` de Python vers Scala.

* Tester des datasets **plus volumineux** (p. ex., Enron emails, Amazon reviews, Wikipedia "dumps").

* Expérimenter **RDD vs. DataFrame**
]

???

Nous concluons avec un petit “devoir” pour la prochaine séance :

- Supprimer le tri et la limitation au top 15, et produire plutôt un nuage de mots (word cloud) avec Python.

- Vous pouvez employer n’importe quelle bibliothèque Python, mais je vous conseille `wordcloud`.
  
La semaine prochaine, nous discuterons de la visualisation que vous aurez implémentée et verrons vos retours.

**Et pour aller plus loin**, je vous suggère :

- De transformer le code Python en Scala, qui est le langage de programmation principal d’Apache Spark.

- D'explorer des ensembles de données plus vastes ou variés, comme les emails Enron, les avis Amazon, ou les dumps de Wikipédia.

- D'expérimenter les API Spark RDD et DataFrame pour comparer leurs performances.

- 


<!-- 
We finish with some homework:

Next week, we will visualize the word clouds you generate from the dataset.

You can use any library you like, but I recommend using the `wordcloud` package.

In addition, you can explore larger or more varied datasets, such as the Enron emails, Amazon reviews, or Wikipedia dumps.

You can also experiment with the RDD vs. DataFrame APIs in Spark.

Finally, you can improve the script to handle stopwords and tokens more efficiently.
-->

---
name: end
layout: true
class: center
---
layout: false
count: false

.center[

<table style="border-collapse: collapse; border: 0; width: 40%; margin: 50px auto">
<tr>
  <td style="border: 0; padding: 0">
    <a href="https://oesteban.github.io/talks/20250410-teaching/">
      <object type="image/svg+xml" data="images/qr-talk-url.svg" style="width: 200px"></object>
      <br />
      Presentation
    </a>
  </td>
  <td style="border: 0; padding: 0">
    <a href="https://github.com/oesteban/talks/tree/main/20250410-teaching">
      <object type="image/svg+xml" data="images/qr-repo-url.svg" style="width: 200px"></object>
      <br />
      Code
    </a>
  </td>
</tr>
</table>

# 302 Data computation

## .green[302.1 — Big data, cloud, infras & HPC (14 ECTS)]

<br />

Oscar Esteban &lt;<code><a href="mailto:phd@oscaresteban.es">phd@oscaresteban.es</a></code>>

#### 10.04.2025
]

???

</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script>
      // Use: ![:img Alt text with spaces but not commas, 50%](image.png)
      remark.macros.img = function (altText, width) {
        var url = this;
        return '<img alt="' + altText + '" src="' + url + '" style="width: ' + width + '" />';
      };
      // Use: ![:video](10.5129/10234)
      remark.macros.video = function (width) {
        var url = this;
        return '<video src="' + url + '" width="' + width + '" preload="auto" controls />';
      };
      // Use: ![:doi](10.5129/10234)
      remark.macros.doi = function () {
        var doi = this;
        return '<a href="https://doi.org/' + doi + '">' + doi + '</a>';
      };

      var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true,
          countIncrementalSlides: false,
          highlightSpans: true,
          ratio: '16:9'
      });

      // Now retrieve all IDs of asciinema casts
      const allcasts = new Map();

      slideshow.on('afterShowSlide', function (slide) {
        // Slide is the slide being navigated
        var slideNumber = slide.getSlideIndex();
        var element = document.getElementsByClassName("remark-visible")[0].getElementsByClassName('asciicast')
        if (element.length == 0 ) {
          return;
        }

        if (allcasts.has(slideNumber)) {
          allcasts.get(slideNumber).play();
          return;
        }

        var castid = element[0].attributes["id"].value;
        allcasts.set(slideNumber, AsciinemaPlayer.create(
            'images/spark.cast',
            document.getElementById('636539'),
            { autoPlay: true, speed: 2, idle_time_limit: 8, rows: 24, cols: 100 }
        ));

      });
      slideshow.on('beforeHideSlide', function (slide) {
        // Slide is the slide being navigated
        var slideNumber = slide.getSlideIndex();
        if (allcasts.has(slideNumber)) {
          allcasts.get(slideNumber).pause();
        }
      });
    </script>
  </body>
</html>
